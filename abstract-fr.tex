La communauté scientifique s'appuie de plus en plus sur les calculs, notamment pour la simulation numérique et le
traitement des données. Alors que de nombreuses avancées scientifiques ont été rendues possibles par les progrès
technologiques des ordinateurs, des gains de performance supplémentaires sont encore nécessaires pour les projets à plus
grande échelle.

La course à la performance est abordée avec une complexité matérielle et logicielle croissante, qui à son tour augmente
la variabilité des performances. Cela peut rendre l'étude expérimentale de la performance extrêmement difficile, ce qui
soulève des préoccupations quant à la reproductibilité des expériences, de manière similaire aux problèmes déjà
rencontrés par les sciences naturelles.

Nos contributions sont doubles. Tout d'abord, nous présentons une méthodologie pour prédire les performances
d'applications parallèles non triviales par la simulation.  Nous décrivons plusieurs modèles de communications et de
calculs, avec une complexité croissante. Nous comparons ces modèles via à une validation approfondie en faisant
correspondre nos prédictions avec des expériences réelles. Cette validation montre que la modélisation de la variabilité
spatiale et temporelle de la plateforme est essentielle pour les prédictions. En conséquence, les prévisions requièrent
une analyse de sensibilité minutieuse tenant compte de l'incertitude sur les modèles de ressources, que nous illustrons
à travers plusieurs études de cas. Par la suite, nous présentons les leçons apprises lors des nombreuses expériences
menées dans la première partie et comment nous avons amélioré notre méthodologie. Nous montrons que les mesures peuvent
souffrir de multiples biais expérimentaux et nous expliquons comment certains de ces biais peuvent être surmontés. Nous
présentons également comment nous avons mis en œuvre des tests systématiques de non-régression des performances, qui
nous ont permis de détecter de nombreux changements significatifs de la plateforme tout au long de cette thèse.
