\chapter{Introduction}
\label{chapter:introduction}

\todo{Make Zenodo repositories for everything relevant and add the links in the thesis. We should have at least: pytree,
peanut, cashew, ratatouille, org\_attach (?), calibration\_analysis, journal\_extract, g5k\_test (*2), hpl,
platform-calibration}
\todo{Remove all the vspace and hspace. Most of them come from the puslished articles, they are not needed here.}


\chapter{Scientific Computing: a story}
\label{chapter:context}

    %% TODO Historique : https://fr.wikipedia.org/wiki/Superordinateur#Historique_des_records

    \section{First computers, from carbon to silicon}%
    \label{sec:first_computers}

        % TODO talk about the human computers, followed by the first (machine) computer created by Cray and IBM, also
        % Von Neuman, analog computers, etc.
        Science has always been tightly associated to computations, hence this is no surprise that the first computers
        were not machines, but humans. Already in the second century AD, Ptolemy, a scientist living in Alexandria,
        wrote the Almagest. This book aggregated the state of the art in mathematics and astronomy and remained a
        reference for centuries. It contained several tables that were computed by the scientist, including a
        trigonometric table (called \emph{table of chords}).

        In 1757, three French astronomers, Clairaut, Lalande and Lepaute, started working on the prediction of the next
        appearances of Halley's comet~\cite[Chapter~1]{human_computers}. Using the recent theories of Newton, they had
        to numerically solve the three-body problem: they computed the orbits of Saturn and Jupiter around the Sun,
        taking into account the attraction force between the two planets. They carried this computation by splitting the
        orbits in tiny steps, computing the new planet locations for each step. They used these sequences of coordinates
        to compute the orbit of Halley's comet around the Sun, by taking into account the effect of the two giant
        planets on the comet and neglecting the effect of the comet itself on the three bodies. They were succesful in
        predicting the next appearance of the comet in the beginning of 1759, making an error of only one month. Their
        work constitutes one of the first recorded division of labor applied to computations, Lalande and Lepaute
        computed the orbits of the two giant planets while Clairaut computed the orbit of the comet.

        Gaspard de Prony, a French civil engineer, went a step further in this endeavor of organized
        computation~\cite[Chapter~2]{human_computers}. In 1791, he was named director of the \emph{Bureau du Cadastre}.
        At the time, the French revolutionary government was preparing reforms for their outdated system of weights and
        measures, which will eventually result in the creation of the metric system. The reforms proposed to measure
        angles in grades instead of degrees, dividing a right angle into 100 grades instead of 90 degrees. Prony was
        tasked to prepare trigonometric tables for this decimal grade system. He organized his staff in a hierarchy of
        three levels:
        \begin{itemize}
            \item The first class of workers, a handful of renowed scientists like Carnot or Legendre, oversaw the
                operations. They had to research the appropriate formulas for computing approximations of trigonometric
                functions with basic arithmetical operations.
            \item The second class, subsequently named \emph{planning commitee}, was a team of eight experienced
                computers. Their task consisted in translating the trigonometric equations produced by the first class
                into a sequence of additions and substractions. They prepared worksheets where all the basic operations
                were written with a blank space for the result.
            \item The third class consisted in nearly ninety computers. Many of them were former servants or wig makers
                that lost their jobs with the Revolution, they did not know any mathematics besides the addition and
                substraction. Their job was to compute the results to fill the blank spaces left by the second class of
                workers.
        \end{itemize}

        The idea of constructing a machine capable of doing computations is not recent. Already in 1642, Blaise Pascal,
        a French mathematician, invented and built a mechanical calculator that could perform the four arithmetical
        operations. The calculator was not commercialy viable at the time, so only twenty machines were built. Much
        later, in the first half of the nineteenth century, Charles Babbage~\cite[Chapters~2-3]{human_computers}, an
        English mathematician, invented two very innovative machines. The difference engine, for computing tables of
        polynomial functions, and the analytical engine, a general purpose computer that would subsequently be qualified
        as \emph{Turing-complete}. Unfortunately, due to a lack of funding, he was never able to build his inventions.
        In the same period, a French inventor named Thomas de Colmar designed and manufactured a digital
        mechanical calculator, called arithmometer. Capable of doing the four arithmetical operations, it was the first
        machine of its kind to be reliable enough for a practical use.  Similarly, Herman Hollerith, an American
        inventor, created the tabulating machine. Initialy built to process the 1890 US Census data, it worked by
        reading and summarizing information stored in punched cards. It decreased considerably the duration and cost of
        the whole census organization. These two last inventions~\cite[Chapter~6]{human_computers} were commercial
        successes and started an era of mechanical computations that lasted until the second half of the twentieth
        century.

        Gradually, computing became more and more important and recognized as a discipline. The apparition of modern
        statistics, mainly due to the work of Francis Galton and Karl Pearson in the early twentieth century, led to a
        growing need for computation power. The First World War itself was an important catalyzer, as the American,
        French and English governments hired entire computing laboratories to create ballistic
        tables~\cite[Chapter~10]{human_computers}. By the time, electromechanical computers were used everywhere, for
        their efficiency and reliability largely superior to human computers. There was still an important need for
        human labor, not only for operating these machines, but some complex operations were still carried by hand.

        The first working general purpose programmable computer, named \emph{Z3}, was designed and built more than two
        decades later in Germany by Konrad Zuse~\cite{sep-computing-history}. Completed in 1941, It was an
        electromechanical machine, using both (mechanical) relays and (electronic) vacuum tubes. Its programs, written
        on external tapes, could use loops but not conditional branches. In 1944, the British government built the first
        fully electronic computer, named \emph{Colossus}. Made of vacuum tubes, its primary function was to break the
        German ciphers during the war. Later, in 1945, the first US electronic computer was unveiled, called
        \emph{ENIAC}. Both the Colossus and ENIAC computers were programmed by plugboards and switches, instead of
        reading from a tape like the Z3. Interestingly, the Z3 and Colossus machines used binary arithmetic while the
        ENIAC used decimal representation.

        An important breakthrough came with the notion of stored-program computer, \ie the idea of storing the program
        instructions in memory. Although the original ideas can be traced to Turing himself and his 1936 article, the
        real implementation in electronical computers came several years later. The first stored-program computer was
        the \emph{Manchester Baby}, built at the University of Manchester in 1948~\cite{sep-computing-history}.
        Similarly, the successor of the ENIAC, called \emph{EDVAC} was also a stored-program computer. Yet, at the time,
        computers were still using vacuum tubes. Although this was a great reliability improvement to the mechanical
        parts used before, the vacuum tubes had a very large electricity consumption which started to become problematic
        (the ENIAC consummed \NSI{150}{\kilo\watt}, the EDVAC \NSI{56}{\kilo\watt}). They also required a large number of
        human operators for their daily usage.

        The invention of the transistor in 1947 by three Bell Labs physicists had a huge impact on the whole electronic
        world. It achieves the same functionnality than a vacuum tube (amplifying and switching an electrical signal),
        but with a much lower power consumption, much smaller size and easier to mass produce. Hence, this is no
        surprise if the industry quickly started to use this new technology. The first transistor computer was made in
        1953 in the University of Manchester. Later, in 1955, IBM announced the first commercial transistor computer,
        the IBM 608~\cite{ibm608}, made of \Num{3000} transistors. Compared to its predecessor, switching to transistor
        allowed IBM to reduce the computer physical size by \NSI{50}{\percent} and its power consumption by
        \NSI{90}{\percent} while multiplying its computing speed by \Num{2.5}, reaching a performance of \Num{4500}
        additions per second.

    \section{Exponential growth}%
    \label{sec:exponential_growth}

        % TODO Moore's law, huge performance gains, helped to get great scientifical breakthrough
        % Not terminated yet, we "need" to go further. Side note: when will it be enough? Question of the environmental
        % impact (we make great progress in terms of efficiency, like flops/W, but the growth is even faster (so for
        % instance the total consumptions are still increasing)).
        In 1965, Gordon Moore, who will later become the CEO and co-founder of Intel, predicted an exponential growth of
        the number of transistors in a chip~\cite{moore:1965}, based on an extrapolation of the current pace of
        technological progress. He estimated that the number of transistors was doubling every year:
        \begin{quote}
            The complexity for minimum component costs has increased at a rate of roughly a factor of two per year.
            Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer
            term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not
            remain nearly constant for at least 10 years.
        \end{quote}

        Ten years later, Moore revised his forecast to a doubling every two year~\cite{moore:1975}. This prediction,
        which revealed to be true, is now known as \emph{Moore's law}.

        \begin{figure}[htbp]
            \centering
            \includegraphics[width=\textwidth]{img/context/49_years.pdf}
            \caption{\label{fig:context:49_years}
            Evolution of the processor characteristics between 1971 and 2020. Plot inspired from the work of Pedro Bruel,
            generated with data from Wikipedia~\cite{wiki2021chronology,wiki2021transistor}.}
        \end{figure}

        One of the main contributions for this exponential growth of the number of transistors is the exponential
        decrease of their size, as plotted in Figure~\ref{fig:context:49_years}. While the IBM 608 calculator
        commercialized in the 50's had transistors ``no bigger than a paper clip''~\cite{ibm608}, the latest processors
        commercialized in 2020 have \NSI{5}{\nano\meter} transistors.

        In 1974, Dennard~\etal listed a set of rules for scaling silmutaneously the transistor density, clock frequency
        and power dissipation of processors~\cite{dennard}, which would eventually be named \emph{Dennard scaling}. The
        effect of this scaling on the device is summarized in Table~\ref{tab:dennard}. A scaling of \(\kappa\) will
        result in a clock frequency multiplied by a factor \(\kappa\) while the power density of the chip remains
        constant, \ie if the size of the processor does not change, it will have the same power consumption and generate
        the same amount of heat.

        \begin{table}[htpb]
            \centering
            \caption{Dennard scaling with a factor \(\kappa\) (table reproduced from~\cite[Table 1]{dennard}).}
            \label{tab:dennard}
            \begin{tabular}{l|c}
                Device or Circuit Parameter & Scaling Factor\\
                \hline
                Device dimension \(t_{ox}, L, W\) & \(1/\kappa\)\\
                Doping concentration \(N_a\) & \(\kappa\)\\
                Voltage \(V\) & \(1/\kappa\)\\
                Current \(I\) & \(1/\kappa\)\\
                Capacitance \(C=\epsilon A/t\) & \(1/\kappa\)\\
                Delay time/circuit \(VC/I\) & \(1/\kappa\)\\
                Power dissipation/circuit \(VI\) & \(1/\kappa^2\)\\
                Power density \(VI/A\) & 1\\
            \end{tabular}
        \end{table}

        Unfortunately, Dennard scaling came to an end 15 years ago. Indeed, it is no longer possible to scale the
        operating voltage and the gate oxide thickness~\cite{Bohr_2007}, as transistors have reached scales where power
        leakage is no longer negligible. With a voltage that cannot be scaled anymore, the power density cannot remain
        constant and reaches alarming levels of more than \NSI{4}{\watt/\milli\metre\squared}

        \begin{figure}[htpb]
            \centering
            \includegraphics[width=\linewidth]{img/context/power_density.pdf}
            \caption{Evolution of the power density in the last 20 years (plot reproduced form ~\cite[Figure
            3]{Hennessy_2019}).}%
            \label{fig:context:power_density}
        \end{figure}

        Consequently, it has became impossible to increase further the CPU frequency, which has reached a limit of about
        \NSI{5}{\giga\hertz} since 2006, as shown by Figure~\ref{fig:context:49_years}.

        TODO: cite~\cite{Hennessy_2019,Hennessy_youtube} and.

        TODO: talk about dark silicon~\cite{Esmaeilzadeh_2011}.


        \begin{figure}[htbp]
            \centering
            \includegraphics[width=\textwidth]{img/context/top500.pdf}
            \caption{\label{fig:context:top500}
            Evolution of the Top500~\cite{top500} supercomputers between 1993 and 2020.  The line denotes the median, the inner ribbon
            contains the \([\NSI{10}{\percent}, \NSI{90}{\percent}]\) interval, the outer ribbon contains all the
            values.\\ Data compiled by Dan Lenski~\cite{top500_compiled} and plotted by ourselves.}
        \end{figure}


    \section{Increased complexity}%
    \label{sec:increased_complexity}

        % TODO increased complexity everywhere (HW and SW), deterministic at micro-level but the macro-level is random
        % no complete understanding of the whole thing, we need experimental CS, we need models (very similarly to what
        % is done in physics or biology)
        Some horror stories~\cite{Petrini_2003}\dots
