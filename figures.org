# -*- coding: utf-8 -*-
# -*- org-confirm-babel-evaluate: nil -*-
# -*- mode: org -*-
#+STARTUP: overview indent inlineimages logdrawer hidestars
* Context
** Top500
*** Downloading the data
The data has been compiled from the Top500 website by Dan Lenski (with a
contribution of Pedro Bruel). This data is used to generate the [[https://commons.wikimedia.org/wiki/File:Processor_families_in_TOP500_supercomputers.svg][Wikipedia]]
figure.

Note that the script is quite fragile. When I first tried it, it was broken, one
of the reasons was that the Top500 now requires a login/password to download the
data. I submitted [[https://github.com/dlenski/top500/issues/2][an issue]] on Github.

#+begin_src sh :results output :exports both
filename=data/context/TOP500_history.csv
wget https://github.com/dlenski/top500/raw/b6c4ddf1777447479757b5bda86ae7228227e331/TOP500_history.csv -O ${filename}
du -sh ${filename}
wc -l ${filename}
#+end_src

#+RESULTS:
: 8,8M	data/context/TOP500_history.csv
: 28001 data/context/TOP500_history.csv
*** Generating the figures
#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(ggplot2)
library(patchwork)
df = read.csv("data/context/TOP500_history.csv")
str(df)
#+end_src

#+RESULTS:
#+begin_example

'data.frame':	28000 obs. of  53 variables:
 $ Year                           : int  1993 1993 1993 1993 1993 1993 1993 1993 1993 1993 ...
 $ Month                          : int  6 6 6 6 6 6 6 6 6 6 ...
 $ Day                            : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Rank                           : num  1 2 3 4 5 6 7 8 9 10 ...
 $ Site                           : Factor w/ 2484 levels " Institute of Information and Communication Technologies at the Bulgarian Academy of Sciences",..: 1296 1390 1525 1498 1528 136 1519 281 473 568 ...
 $ Manufacturer                   : Factor w/ 148 levels "Acer Group","ACTION",..: 142 142 142 142 98 98 142 76 28 28 ...
 $ Computer                       : Factor w/ 2907 levels " eServer pSeries 655 (1.7 GHz Power4+)",..: 806 815 814 814 2413 2412 812 1017 2812 2812 ...
 $ Country                        : Factor w/ 63 levels "Australia","Austria",..: 61 61 61 61 27 7 61 61 61 61 ...
 $ Processors                     : num  1024 544 512 512 4 ...
 $ RMax                           : num  59.7 30.4 30.4 30.4 23.2 20 15.1 13.9 13.7 13.7 ...
 $ RPeak                          : num  131 69.6 65.5 65.5 25.6 ...
 $ Nmax                           : num  52224 36864 36864 36864 6400 ...
 $ Nhalf                          : num  24064 16384 16384 16384 830 ...
 $ Processor.Family               : Factor w/ 26 levels "","Alpha","AMD",..: 25 25 25 25 21 21 25 14 7 7 ...
 $ Processor                      : Factor w/ 484 levels "A64FX 48C 2.2GHz",..: 275 275 275 275 141 141 275 70 31 31 ...
 $ Processor.Speed..MHz.          : num  32 32 32 32 400 ...
 $ System.Family                  : Factor w/ 185 levels " IBM Power Systems",..: 180 180 180 180 124 124 180 106 34 34 ...
 $ Operating.System               : Factor w/ 93 levels "AIX","Amazon Linux 2",..: 12 12 12 12 65 65 12 34 81 81 ...
 $ Architecture                   : Factor w/ 6 levels "Cluster","Constellations",..: 3 3 3 3 6 6 3 3 6 6 ...
 $ Segment                        : Factor w/ 7 levels "Academic","Classified",..: 6 4 1 2 7 6 6 1 7 6 ...
 $ Application.Area               : Factor w/ 48 levels "","Aerospace",..: 37 37 37 37 37 46 37 37 37 37 ...
 $ Interconnect.Family            : Factor w/ 27 levels "10G","Cray Interconnect",..: 8 8 8 8 3 3 8 17 17 17 ...
 $ Interconnect                   : Factor w/ 100 levels "100G Ethernet",..: 39 39 39 39 75 75 39 79 79 79 ...
 $ Region                         : Factor w/ 17 levels "","Australia and New Zealand",..: 7 7 7 7 5 7 7 7 7 7 ...
 $ Continent                      : Factor w/ 7 levels "Africa","Americas",..: 2 2 2 2 3 2 2 2 2 2 ...
 $ Power                          : num  NA NA NA NA NA NA NA NA NA NA ...
 $ System.Model                   : Factor w/ 564 levels "","Acer AR585 F1 Cluster",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Total.Cores                    : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Measured.Size                  : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Processor.Cores                : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Accelerator                    : Factor w/ 7 levels "","ATI GPU","IBM PowerXCell 8i",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Name                           : Factor w/ 898 levels "","1","2","A1",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Accelerator.Cores              : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Efficiency....                 : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Mflops.Watt                    : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Processor.Technology           : Factor w/ 28 levels "","AMD x86_64",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ OS.Family                      : Factor w/ 39 levels "","Amazon Linux 2",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Cores.per.Socket               : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Processor.Generation           : Factor w/ 76 levels "","AMD Naples",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Previous.Rank                  : num  NA NA NA NA NA NA NA NA NA NA ...
 $ First.Appearance               : num  NA NA NA NA NA NA NA NA NA NA ...
 $ First.Rank                     : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Accelerator.Co.Processor.Cores : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Accelerator.Co.Processor       : Factor w/ 64 levels "","AMD FirePro S10000",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Power.Source                   : Factor w/ 4 levels "","Derived","Optimized",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Rmax..TFlop.s.                 : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Rpeak..TFlop.s.                : num  NA NA NA NA NA NA NA NA NA NA ...
 $ HPCG..TFlop.s.                 : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Power..kW.                     : num  NA NA NA NA NA NA NA NA NA NA ...
 $ Power.Effeciency..GFlops.Watts.: num  NA NA NA NA NA NA NA NA NA NA ...
 $ Site.ID                        : num  NA NA NA NA NA NA NA NA NA NA ...
 $ System.ID                      : int  NA NA NA NA NA NA NA NA NA NA ...
 $ Power.Efficiency..GFlops.Watts.: num  NA NA NA NA NA NA NA NA NA NA ...
#+end_example

There are several columns to denote a number of cores (total, only cpu, only
accelerators...).
#+begin_src R :results output :session *R* :exports both
df %>% filter(Year == 2020, Month==11, Rank %in% c(2,3,5)) %>% select(Site, Processor, Accelerator.Co.Processor, Total.Cores, Processor.Cores, Accelerator.Co.Processor.Cores, Accelerator.Cores)
#+end_src

#+RESULTS:
#+begin_example
                                  Site                 Processor
1 DOE/SC/Oak Ridge National Laboratory    IBM POWER9 22C 3.07GHz
2                        DOE/NNSA/LLNL     IBM POWER9 22C 3.1GHz
3                   NVIDIA Corporation AMD EPYC 7742 64C 2.25GHz
  Accelerator.Co.Processor Total.Cores Processor.Cores
1       NVIDIA Volta GV100     2414592              NA
2       NVIDIA Volta GV100     1572480              NA
3              NVIDIA A100      555520              NA
  Accelerator.Co.Processor.Cores Accelerator.Cores
1                        2211840                NA
2                        1382400                NA
3                         483840                NA
#+end_example

So, let's unify and clean the data.
#+begin_src R :results output :session *R* :exports both
df = df %>%
    mutate(date=ISOdate(Year, Month, Day)) %>%
    mutate(perf_hpl=ifelse(is.na(RMax), Rmax..TFlop.s.*1000, RMax)) %>%
    mutate(perf_theoretical=ifelse(is.na(RPeak), Rpeak..TFlop.s.*1000, RMax)) %>%
    mutate(perf_hpcg=HPCG..TFlop.s.*1000) %>%
    mutate(efficiency=perf_hpl/perf_theoretical) %>%
    mutate(total_cores=ifelse(is.na(Total.Cores), Processors, Total.Cores)) %>%
    mutate(accelerator_cores=ifelse(is.na(Accelerator.Cores), 0, Accelerator.Cores)) %>%
    mutate(accelerator_cores=ifelse(is.na(Accelerator.Co.Processor.Cores), accelerator_cores, Accelerator.Co.Processor.Cores)) %>%
    mutate(cpu_cores=total_cores-accelerator_cores) %>%
    mutate(proc_freq=Processor.Speed..MHz.)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
df %>% filter(Year == 2020, Month==11, Rank <= 10) %>% select(Site, total_cores, cpu_cores, accelerator_cores)
#+end_src

#+RESULTS:
#+begin_example
                                             Site total_cores cpu_cores
1          RIKEN Center for Computational Science     7630848   7630848
2            DOE/SC/Oak Ridge National Laboratory     2414592    202752
3                                   DOE/NNSA/LLNL     1572480    190080
4          National Supercomputing Center in Wuxi    10649600  10649600
5                              NVIDIA Corporation      555520     71680
6     National Super Computer Center in Guangzhou     4981760    427008
7                 Forschungszentrum Juelich (FZJ)      449280     44928
8                                      Eni S.p.A.      669760     87360
9  Texas Advanced Computing Center/Univ. of Texas      448448    448448
10                                   Saudi Aramco      672520     39560
   accelerator_cores
1                  0
2            2211840
3            1382400
4                  0
5             483840
6            4554752
7             404352
8             582400
9                  0
10            632960
#+end_example

#+begin_src R :results output :session *R* :exports both
group_top500 = function(df, y_col, group_col, q) {
    return (df %>%
        group_by(.data[[group_col]]) %>%
        filter(!any(is.na(.data[[y_col]]))) %>%
        summarise(min_val=min(.data[[y_col]]),
                  small_val=quantile(.data[[y_col]], q),
                  med_val=median(.data[[y_col]]),
                  large_val=quantile(.data[[y_col]], 1-q),
                  max_val=max(.data[[y_col]])
        )
    )
}
plot_top500 = function(df, y_col, x_col, color) {
    q = 0.1
    df %>% group_top500(y_col, x_col, q) %>%
        ggplot() +
        aes_string(x=x_col, ymin="min_val", ymax="max_val", y="med_val") +
        geom_line(color=color) +
        geom_ribbon(alpha=0.3, fill=color) +
        geom_ribbon(aes_string(ymin="small_val", ymax="large_val"), alpha=0.3, fill=color) +
        theme_light() -> plot
    return(plot)
}
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
p_cores = plot_top500(df, "total_cores",  "date", "#1b9e77") +
    scale_y_log10(breaks=c(1e1, 1e3, 1e5, 1e7)) +
    ggtitle("Number of cores")
p_freq  = plot_top500(df, "proc_freq", "date", "#d95f02") +
    scale_y_log10(breaks=c(8, 40, 200, 1000, 5000)) +
    ggtitle("CPU frequency (MHz)")
p_perf  = plot_top500(df, "perf_hpl",  "date", "#7570b3") +
    scale_y_log10(breaks=c(1e0, 1e2, 1e4, 1e6, 1e8)) +
    ggtitle("Performance (Gflop/s)")
min_year = df %>% pull(Year) %>% min()
max_year = df %>% pull(Year) %>% max()
plot = (p_cores + p_freq + p_perf & 
    theme(axis.title.x=element_blank(),
          axis.title.y=element_blank()
        )
    )
ggsave(filename='img/context/top500.pdf', plot=plot, width=7, height=3)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-REZPeJ/figureOTRz3J.png]]


#+begin_src R :results output :session *R* :exports both
df %>%
    filter(proc_freq >= 3500) %>%
    select(Processor, proc_freq) %>%
    unique() %>%
    arrange(-proc_freq)
#+end_src

#+RESULTS:
#+begin_example

                        Processor proc_freq
1                          POWER6      5000
2                          POWER6      4700
3               POWER6 2C 4.70GHz      4700
4              POWER6 2C 4.700GHz      4700
5                   PowerXCell 8i      4000
6                          POWER7      3860
7               POWER7 8C 3.86GHz      3860
8                          POWER7      3836
9              POWER7 8C 3.836GHz      3836
10              POWER7 8C 3.84GHz      3836
11              POWER7 8C 3.83GHz      3830
12         Intel EM64T Xeon EM64T      3600
13     Intel IA-32 Pentium 4 Xeon      3600
14            Xeon EM64T  3.60GHz      3600
15       Xeon E3-1280v3 4C 3.6GHz      3600
16       Xeon Gold 5122 4C 3.6GHz      3600
17          IBM POWER9 20C 3.6GHz      3600
18             POWER7  8C 3.55GHz      3550
19 Intel Xeon E5-2637v2 4C 3.5GHz      3500
#+end_example
** 49 years of micro-processor data
*** Downloading the data
The data comes from two Wikipedia pages:
- [[https://en.wikipedia.org/wiki/Transistor_count][Transistor count]]
- [[https://en.wikipedia.org/wiki/Microprocessor_chronology][Microprocessor chronology]]

Here we use Python and Pandas to download the data.
**** Transistor count
#+begin_src python :results output :session *python* :exports both
import pandas
print(f'Pandas version {pandas.__version__}')

df_list = pandas.read_html('https://en.wikipedia.org/wiki/Transistor_count', match='Intel 8008')
assert len(df_list) == 1
df = df_list[0]
df.columns = ['processor', 'transistor_count', 'date', 'designer', 'process', 'area']

def grep_number(column):
    return column.astype(str).str.extract('([0-9,\,]+)')[0].str.replace(',', '')

def parse_number(df, colname, number_type=float):
    df[colname] = grep_number(df[colname]).astype(number_type)

parse_number(df, 'transistor_count', float)
parse_number(df, 'date', int)
parse_number(df, 'process', float)
parse_number(df, 'area', float)
print(f'Dataframe of {len(df)} rows and {len(df.columns)} columns')
print(df.head())
df.to_csv('data/context/transistor_count.csv', index=false)
#+end_src

#+RESULTS:
: Pandas version 1.2.1
: Dataframe of 191 rows and 6 columns
:                                 processor  transistor_count  date            designer  process  area
: 0  MP944 (20-bit, 6-chip, 28 chips total)           74442.0  1970  Garrett AiResearch      NaN   NaN
: 1              Intel 4004 (4-bit, 16-pin)            2250.0  1971               Intel  10000.0  12.0
: 2                TMX 1795 (?-bit, 24-pin)            3078.0  1971   Texas Instruments      NaN  30.0
: 3              Intel 8008 (8-bit, 18-pin)            3500.0  1972               Intel  10000.0  14.0
: 4              NEC μCOM-4 (4-bit, 42-pin)            2500.0  1973                 NEC   7500.0   NaN
**** Microprocessor chronology
#+begin_src python :results output :session *python* :exports both
import pandas
print(f'Pandas version {pandas.__version__}')

df_list = pandas.read_html('https://en.wikipedia.org/wiki/Microprocessor_chronology', match='Developer')
assert len(df_list) == 6

for df in df_list:
    columns = list(df.columns)
    new_cols = ['date', 'processor', 'designer', 'frequency',]
    columns[:len(new_cols)] = new_cols
    df.columns = columns

df = pandas.concat(df_list).reset_index(drop=True)

def grep_number(column):
    return column.astype(str).str.extract('([0-9,\,\.]+)')[0].str.replace(',', '')

def parse_number(df, colname, number_type=float):
    df[colname] = grep_number(df[colname]).astype(number_type)

parse_number(df, 'date', int)

parse_number(df, 'Word size(bits)', float)
df['word_size'] = df['Word size(bits)']
df.loc[df['date'] >= 2000, 'word_size'] = 64  # after 2000, all the processors listed here are 64 bits

parse_number(df, 'Transistors', float)
parse_number(df, 'Transistors(millions)', float)
df['transistor_count'] = float('nan')
df.loc[~df['Transistors'].isna(), 'transistor_count'] = df['Transistors']
df.loc[~df['Transistors(millions)'].isna(), 'transistor_count'] = df['Transistors(millions)']*1e6

def parse_unit(df, colname, unit, new_name):
    unit_prefix = {'n':1e-9, 'μ':1e-6, 'm':1e-3, 'k':1e3, 'M':1e6, 'G':1e9}
    number = '[0-9]+(?:\.[0-9]+)?'
    reg = f'({number})(?:(?:-|–)({number}))?\s*([{"|".join(unit_prefix)}]){unit}'
    result = df[colname].astype(str).str.replace(',', '').str.extract(reg)
    result['multiplier'] = result.apply(lambda row: unit_prefix.get(row[2], float('nan')), axis=1)
    result.loc[result[1].isna(), 1] = result[0]
    result[0] = result[0].astype(float) * result['multiplier']
    result[1] = result[1].astype(float) * result['multiplier']
    df[f'{new_name}_min'] = result[[0, 1]].min(axis=1)
    df[f'{new_name}_max'] = result[[0, 1]].max(axis=1)

parse_unit(df, 'frequency', 'Hz', 'frequency')
parse_unit(df, 'Process', 'm', 'process')

def parse_cores(val):
    NA = float('nan')
    if isinstance(val, float):  # NaN
        return NA, NA, NA, NA
    val = val.split('/')
    if len(val) == 1:
        val.append('')
    def parse_list(list_str):
        if '–' in list_str:
            values = list_str.split('–')
        else:
            values = list_str.split(',')
        if values == ['']:
            return [NA]
        else:
            for i, val in enumerate(values):
                try:
                    a, b = val.split('+')
                    values[i] = int(a) + int(b)
                except ValueError:
                    values[i] = int(val)
            return values
    cores = parse_list(val[0])
    dies = parse_list(val[1])
    return min(cores), max(cores), min(dies), max(dies)

# Testcases for this ugly function
assert str(parse_cores('6, 8, 12, 16, 24, 32, 64 / 1, 2, 4')) == '(6, 64, 1, 4)'
assert str(parse_cores('6, 8, 12, 16 /')) == '(6, 16, nan, nan)'
assert str(parse_cores('32')) == '(32, 32, nan, nan)'
assert str(parse_cores('1+8 / 1')) == '(9, 9, 1, 1)'
assert str(parse_cores('4–6 / 1')) == '(4, 6, 1, 1)'

df[['core_per_die_min', 'core_per_die_max', 'die_per_module_min', 'die_per_module_max']] = df.apply(lambda row: pandas.Series(parse_cores(row['Cores per die /Dies per module'])), axis=1)

df = df[['date', 'processor', 'designer', 'word_size', 'transistor_count', 'frequency_min',
       'frequency_max', 'process_min', 'process_max', 'core_per_die_min',
       'core_per_die_max', 'die_per_module_min', 'die_per_module_max']]

print(f'Dataframe of {len(df)} rows and {len(df.columns)} columns')
print(df.head())
df.to_csv('data/context/microprocessor_chronology.csv', index=False)
#+end_src

#+RESULTS:
: Pandas version 1.2.1
: Dataframe of 212 rows and 13 columns
:    date processor   designer  word_size  transistor_count  frequency_min  frequency_max  process_min  process_max  core_per_die_min  core_per_die_max  die_per_module_min  die_per_module_max
: 0  1971      4004      Intel        4.0            2250.0       740000.0       740000.0      0.00001      0.00001               NaN               NaN                 NaN                 NaN
: 1  1972    PPS-25  Fairchild        4.0               NaN       400000.0       400000.0          NaN          NaN               NaN               NaN                 NaN                 NaN
: 2  1972    μPD700        NEC        4.0               NaN            NaN            NaN          NaN          NaN               NaN               NaN                 NaN                 NaN
: 3  1972      8008      Intel        8.0            3500.0       500000.0       500000.0      0.00001      0.00001               NaN               NaN                 NaN                 NaN
: 4  1972     PPS-4   Rockwell        4.0               NaN       200000.0       200000.0          NaN          NaN               NaN               NaN                 NaN                 NaN
*** Generating the figure
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
library(tidyr)
library(dplyr)
library(ggplot2)
library(scales)

plot = bind_rows(
        read.csv('data/context/microprocessor_chronology.csv'),
        read.csv('data/context/transistor_count.csv') %>% mutate(core_per_die_min=-42, core_per_die_max=-42)  # dirty hack to remove these points
    ) %>%
    mutate(process_min=process_min*1e9, process_max=process_max*1e9) %>%
    mutate(frequency_min=frequency_min*1e-6, frequency_max=frequency_max*1e-6) %>%
    mutate(transistor_count = transistor_count*1e-3) %>%
    replace_na(list(core_per_die_min=1, core_per_die_max=1)) %>%
    select(-processor, -designer, -word_size, -die_per_module_min, -die_per_module_max, -area) %>%
    gather(key, value, -date) %>%
    mutate(key=ifelse(grepl("frequency", key), "Frequency (MHz)", key)) %>%
    mutate(key=ifelse(grepl("process", key), "Process (nm)", key)) %>%
    mutate(key=ifelse(grepl("core_per_die", key), "Cores (count)", key)) %>%
    mutate(key=ifelse(grepl("transistor_count", key), "Transistors (thousands)", key)) %>%
    ggplot() +
        aes(x=date, y=value, color=key, shape=key) +
        geom_point(size=2) +
        theme_bw() +
        scale_color_brewer(palette="Dark2") +
        scale_shape_manual(values=c(0,1,2,6)) +
        scale_y_log10(breaks=10^seq(-1,7,2)) +
    #    scale_y_log10(breaks = trans_breaks(trans = "log10",
    #                                    inv = function(x) 10 ^ x,
    #                                    n = 7),
    #              labels = trans_format("log10", math_format(10^.x))) +
        xlab('Year') + 
        theme(axis.title.y = element_blank()) +
        theme(legend.position = c(0.15, 0.81), legend.title=element_blank()) +
        theme(legend.background=element_rect(color="black"))
ggsave(filename='img/context/49_years.pdf', plot=plot, width=7, height=4)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-REZPeJ/figurerSwaSp.png]]

* Performance prediction through simulation
** Modeling HPL kernels and communications
*** Kernels plots                                                :noexport:
**** Downloading the CSV
The file trace_functions.csv has been generated with this [[https://github.com/Ezibenroc/mpi_calibration/blob/74870b0d26497cf623c47a747e2f089eedb62857/dahu/hpl/hpl_trace_simple.ipynb][notebook]] (the dump to
the CSV file happens in cells 25-26). The notebook uses this [[https://github.com/Ezibenroc/mpi_calibration/blob/74870b0d26497cf623c47a747e2f089eedb62857/dahu/smpi_hpl/grenoble_2019-04-03_1858209.zip][ZIP archive]].

The file dgemm_calibration.csv has been generated with this [[https://github.com/Ezibenroc/mpi_calibration/blob/df5a957901fac35a3df0bd466acea7d6199a9426/dahu/blas/dgemm_heterogeneous_model.ipynb][notebook]] (the dump
to the CSV file happens in cell 38). The notebook uses these [[https://github.com/Ezibenroc/mpi_calibration/tree/26fdfbb565e1eb5b9f1015a47ddd8fe9aaa424e5/dahu/blas/heterogeneity_exp/7][ZIP archives]].

#+begin_src sh :results output :exports both
mkdir -p data/prediction/modeling/kernels/
cd data/prediction/modeling/kernels/
wget -c https://github.com/Ezibenroc/mpi_calibration/raw/master/dahu/smpi_hpl/paper_sc19/traces/2/trace_functions.csv
sed 's/function/func/g' -i trace_functions.csv  # cannot have a column named "function" in R...
wget -c https://github.com/Ezibenroc/mpi_calibration/raw/master/dahu/blas/dgemm_calibration.csv
sed 's/function/func/g' -i dgemm_calibration.csv  # cannot have a column named "function" in R...
#+end_src

#+RESULTS:

**** Drawing the regression plots
There are several interesting functions in the CSV file. For each function,
there are real observations *and* predictions, the column "mode" can be used to
distinguish them.
***** DGEMM from a calibration
#+begin_src R :results output :session *R* :exports both
library(ggplot2)
options(crayon.enabled = FALSE)
df = read.csv('data/prediction/modeling/kernels/dgemm_calibration.csv')
str(df)
#+end_src

#+RESULTS:
#+begin_example

'data.frame':	5004289 obs. of  14 variables:
 $ func        : Factor w/ 2 levels "6","dgemm": 2 2 2 2 2 2 2 2 2 2 ...
 $ m           : int  378 378 378 9441 9441 9441 1041 1041 1041 1248 ...
 $ n           : int  7640 7640 7640 640 640 640 2183 2183 2183 1343 ...
 $ k           : int  2427 2427 2427 1160 1160 1160 735 735 735 1991 ...
 $ timestamp   : num  3473 3474 3474 3475 3475 ...
 $ duration    : num  0.486 0.486 0.487 0.455 0.454 ...
 $ prediction  : num  0.522 0.522 0.522 0.485 0.485 ...
 $ noise       : num  0.000512 -0.004775 0.001385 -0.001869 0.004448 ...
 $ pred_noise  : num  0.522 0.517 0.523 0.483 0.489 ...
 $ node        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ core        : int  0 0 0 0 0 0 0 0 0 0 ...
 $ cpu         : int  20 20 20 20 20 20 20 20 20 20 ...
 $ index       : int  0 1 2 3 4 5 6 7 8 9 ...
 $ index_in_seq: int  0 1 2 0 1 2 0 1 2 0 ...
#+end_example

#+begin_src R :results output :session *R* :exports both
## df$node = 1 + df$rank %/% 32
## df$cpu = 2*df$node + df$rank %% 2
df$m = as.numeric(df$m)
df$n = as.numeric(df$n)
df$k = as.numeric(df$k)
## df$mnk = df$m * df$n * df$k
## df$mn = df$m * df$n
## df$mk = df$m * df$k
## df$nk = df$n * df$k
str(df)
head(df)
#+end_src

#+RESULTS:
#+begin_example

'data.frame':	5004289 obs. of  14 variables:
 $ func        : Factor w/ 2 levels "6","dgemm": 2 2 2 2 2 2 2 2 2 2 ...
 $ m           : num  378 378 378 9441 9441 ...
 $ n           : num  7640 7640 7640 640 640 ...
 $ k           : num  2427 2427 2427 1160 1160 ...
 $ timestamp   : num  3473 3474 3474 3475 3475 ...
 $ duration    : num  0.486 0.486 0.487 0.455 0.454 ...
 $ prediction  : num  0.522 0.522 0.522 0.485 0.485 ...
 $ noise       : num  0.000512 -0.004775 0.001385 -0.001869 0.004448 ...
 $ pred_noise  : num  0.522 0.517 0.523 0.483 0.489 ...
 $ node        : int  10 10 10 10 10 10 10 10 10 10 ...
 $ core        : int  0 0 0 0 0 0 0 0 0 0 ...
 $ cpu         : int  20 20 20 20 20 20 20 20 20 20 ...
 $ index       : int  0 1 2 3 4 5 6 7 8 9 ...
 $ index_in_seq: int  0 1 2 0 1 2 0 1 2 0 ...

   func    m    n    k timestamp  duration prediction         noise pred_noise
1 dgemm  378 7640 2427  3473.428 0.4859466  0.5217815  0.0005118576  0.5222933
2 dgemm  378 7640 2427  3473.914 0.4861293  0.5217815 -0.0047750420  0.5170064
3 dgemm  378 7640 2427  3474.401 0.4868529  0.5217815  0.0013853568  0.5231668
4 dgemm 9441  640 1160  3474.887 0.4551385  0.4845474 -0.0018686303  0.4826788
5 dgemm 9441  640 1160  3475.343 0.4535278  0.4845474  0.0044477582  0.4889952
6 dgemm 9441  640 1160  3475.796 0.4544535  0.4845474  0.0007154680  0.4852629
  node core cpu index index_in_seq
1   10    0  20     0            0
2   10    0  20     1            1
3   10    0  20     2            2
4   10    0  20     3            0
5   10    0  20     4            1
6   10    0  20     5            2
#+end_example

#+begin_src R :results output :session *R* :exports both
unique(df$node)
unique(df$cpu)
#+end_src

#+RESULTS:
:  [1] 10 26 31  3 13 18  6  7 29  8  2 20 16  9 23 15 32 22 14 19 12 25 30 17 24
: [26] 11  1  5  4 28 21 27 NA
: 
:  [1] 20 21 52 53 62 63  6  7 26 27 36 37 12 13 14 15 58 59 16 17  4  5 40 41 32
: [26] 33 18 19 46 47 30 31 64 65 44 45 28 29 38 39 24 25 50 51 60 61 34 35 48 49
: [51] 22 23  2  3 10 11  8  9 56 57 42 43 54 55 NA

#+begin_src R :results output :session *R* :exports both
set.seed(42)
dgemm = df[sample(nrow(df), 100000), ] # This is too large to be plotted
dgemm = dgemm[dgemm$m*dgemm$n*dgemm$k<1.2E10,]
summary(lm(data=dgemm, duration ~ I(m*n*k):factor(cpu)))
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = duration ~ I(m * n * k):factor(cpu), data = dgemm)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.11117 -0.01051 -0.00431  0.00458  0.40917 

Coefficients:
                            Estimate Std. Error t value Pr(>|t|)    
(Intercept)                7.310e-03  1.908e-04   38.31   <2e-16 ***
I(m * n * k):factor(cpu)2  6.805e-11  1.310e-13  519.42   <2e-16 ***
I(m * n * k):factor(cpu)3  6.609e-11  1.272e-13  519.34   <2e-16 ***
I(m * n * k):factor(cpu)4  6.667e-11  1.302e-13  512.27   <2e-16 ***
I(m * n * k):factor(cpu)5  6.560e-11  1.296e-13  506.08   <2e-16 ***
I(m * n * k):factor(cpu)6  6.637e-11  1.306e-13  508.08   <2e-16 ***
I(m * n * k):factor(cpu)7  6.563e-11  1.298e-13  505.58   <2e-16 ***
I(m * n * k):factor(cpu)8  6.599e-11  1.266e-13  521.10   <2e-16 ***
I(m * n * k):factor(cpu)9  6.576e-11  1.306e-13  503.55   <2e-16 ***
I(m * n * k):factor(cpu)10 6.640e-11  1.305e-13  508.67   <2e-16 ***
I(m * n * k):factor(cpu)11 6.508e-11  1.337e-13  486.62   <2e-16 ***
I(m * n * k):factor(cpu)12 6.807e-11  1.309e-13  520.15   <2e-16 ***
I(m * n * k):factor(cpu)13 6.529e-11  1.317e-13  495.90   <2e-16 ***
I(m * n * k):factor(cpu)14 6.572e-11  1.325e-13  496.01   <2e-16 ***
I(m * n * k):factor(cpu)15 6.498e-11  1.293e-13  502.76   <2e-16 ***
I(m * n * k):factor(cpu)16 6.709e-11  1.329e-13  504.68   <2e-16 ***
I(m * n * k):factor(cpu)17 6.543e-11  1.342e-13  487.40   <2e-16 ***
I(m * n * k):factor(cpu)18 6.550e-11  1.306e-13  501.61   <2e-16 ***
I(m * n * k):factor(cpu)19 6.558e-11  1.327e-13  494.27   <2e-16 ***
I(m * n * k):factor(cpu)20 6.562e-11  1.296e-13  506.45   <2e-16 ***
I(m * n * k):factor(cpu)21 6.551e-11  1.288e-13  508.69   <2e-16 ***
I(m * n * k):factor(cpu)22 6.544e-11  1.297e-13  504.54   <2e-16 ***
I(m * n * k):factor(cpu)23 6.613e-11  1.302e-13  508.08   <2e-16 ***
I(m * n * k):factor(cpu)24 6.564e-11  1.324e-13  495.96   <2e-16 ***
I(m * n * k):factor(cpu)25 6.533e-11  1.322e-13  494.35   <2e-16 ***
I(m * n * k):factor(cpu)26 8.705e-11  1.276e-13  682.32   <2e-16 ***
I(m * n * k):factor(cpu)27 7.166e-11  1.283e-13  558.52   <2e-16 ***
I(m * n * k):factor(cpu)28 7.356e-11  1.230e-13  598.02   <2e-16 ***
I(m * n * k):factor(cpu)29 7.328e-11  1.323e-13  553.74   <2e-16 ***
I(m * n * k):factor(cpu)30 8.030e-11  1.288e-13  623.48   <2e-16 ***
I(m * n * k):factor(cpu)31 7.971e-11  1.264e-13  630.41   <2e-16 ***
I(m * n * k):factor(cpu)32 7.545e-11  1.247e-13  604.92   <2e-16 ***
I(m * n * k):factor(cpu)33 7.449e-11  1.263e-13  589.99   <2e-16 ***
I(m * n * k):factor(cpu)34 7.009e-11  1.296e-13  540.86   <2e-16 ***
I(m * n * k):factor(cpu)35 6.589e-11  1.297e-13  507.83   <2e-16 ***
I(m * n * k):factor(cpu)36 6.933e-11  1.264e-13  548.65   <2e-16 ***
I(m * n * k):factor(cpu)37 6.552e-11  1.311e-13  499.63   <2e-16 ***
I(m * n * k):factor(cpu)38 6.570e-11  1.310e-13  501.48   <2e-16 ***
I(m * n * k):factor(cpu)39 6.554e-11  1.316e-13  498.06   <2e-16 ***
I(m * n * k):factor(cpu)40 6.615e-11  1.279e-13  517.26   <2e-16 ***
I(m * n * k):factor(cpu)41 6.568e-11  1.331e-13  493.49   <2e-16 ***
I(m * n * k):factor(cpu)42 6.607e-11  1.280e-13  516.20   <2e-16 ***
I(m * n * k):factor(cpu)43 6.569e-11  1.283e-13  511.92   <2e-16 ***
I(m * n * k):factor(cpu)44 6.533e-11  1.282e-13  509.67   <2e-16 ***
I(m * n * k):factor(cpu)45 6.583e-11  1.271e-13  517.80   <2e-16 ***
I(m * n * k):factor(cpu)46 6.590e-11  1.281e-13  514.52   <2e-16 ***
I(m * n * k):factor(cpu)47 6.492e-11  1.319e-13  492.34   <2e-16 ***
I(m * n * k):factor(cpu)48 6.548e-11  1.288e-13  508.20   <2e-16 ***
I(m * n * k):factor(cpu)49 6.550e-11  1.303e-13  502.68   <2e-16 ***
I(m * n * k):factor(cpu)50 6.994e-11  1.325e-13  527.90   <2e-16 ***
I(m * n * k):factor(cpu)51 6.483e-11  1.305e-13  496.83   <2e-16 ***
I(m * n * k):factor(cpu)52 6.571e-11  1.310e-13  501.66   <2e-16 ***
I(m * n * k):factor(cpu)53 6.616e-11  1.308e-13  505.94   <2e-16 ***
I(m * n * k):factor(cpu)54 6.583e-11  1.277e-13  515.62   <2e-16 ***
I(m * n * k):factor(cpu)55 6.608e-11  1.286e-13  513.77   <2e-16 ***
I(m * n * k):factor(cpu)56 6.734e-11  1.305e-13  515.94   <2e-16 ***
I(m * n * k):factor(cpu)57 6.557e-11  1.291e-13  507.92   <2e-16 ***
I(m * n * k):factor(cpu)58 6.589e-11  1.301e-13  506.30   <2e-16 ***
I(m * n * k):factor(cpu)59 6.552e-11  1.281e-13  511.38   <2e-16 ***
I(m * n * k):factor(cpu)60 6.611e-11  1.312e-13  504.02   <2e-16 ***
I(m * n * k):factor(cpu)61 6.552e-11  1.317e-13  497.58   <2e-16 ***
I(m * n * k):factor(cpu)62 6.552e-11  1.298e-13  504.67   <2e-16 ***
I(m * n * k):factor(cpu)63 6.559e-11  1.326e-13  494.52   <2e-16 ***
I(m * n * k):factor(cpu)64 6.539e-11  1.307e-13  500.16   <2e-16 ***
I(m * n * k):factor(cpu)65 6.561e-11  1.277e-13  513.64   <2e-16 ***
---
codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.02955 on 99770 degrees of freedom
Multiple R-squared:  0.978,	Adjusted R-squared:  0.978 
F-statistic: 6.928e+04 on 64 and 99770 DF,  p-value: < 2.2e-16
#+end_example

Regression lines, to show the heterogeneity.
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 400 :height 400 :session *R* 
plot = ggplot(dgemm, aes(x=m*n*k, y=duration, color=factor(cpu%%9))) +
    geom_point(alpha=.1,size=.5) + 
    geom_smooth(data=dgemm, aes(group=factor(cpu)), method='lm', se=F, fullrange=T, size=.5) +
    geom_smooth(data=dgemm, color="black", method='lm', se=F, fullrange=T, linetype=4) +
    scale_color_brewer(palette="Set1", guide=F)
plot = plot + theme_bw() + ylab('Duration (s)') + 
    scale_x_continuous(name = 'M.N.K', breaks = (0:4)*3E9, limits=c(0,1.2E10)) + 
    labs(color='CPU') 
ggsave(filename='img/prediction/modeling/kernels/dgemm_heterogeneity_calib.png', plot=plot, width=7,height=3, dpi=200)
## ggsave(filename="figures/kernels/dgemm_heterogeneity.pdf", plot=plot, width=4,height=4)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figureV0kXXO.png]]


#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(tidyr)
library(forcats)

dgemm2 = dgemm[(dgemm$cpu ==2),]
dgemm2 %>% select(func,m,n,k,duration,prediction,pred_noise,node,core,cpu) %>%
    gather(duration,prediction,pred_noise,key=type,value=duration) -> dgemm2
fake_dgemm = dgemm2[1,] # This is just to add a black legend for geom_smooth
fake_dgemm$m = 695 # 0 if even values of mnk_id are selected
fake_dgemm$n = 695 # 0
fake_dgemm$k = 695 # 0 
fake_dgemm$type = "fake"
fake_dgemm$duration = 0
dgemm2 = rbind(fake_dgemm, dgemm2)
dgemm2 %>%
    mutate(type = fct_recode(
               fct_relevel(type, "duration", "fake", "prediction", "pred_noise"),
               "Reality"="duration", 
               "M1/N0 (linear)"="fake", 
               "M2/N0 (polynomial)"="prediction",
               "M2/N2 (polynomial + noise)"="pred_noise")) -> dgemm2
dgemm2 %>% mutate(mnk=m*n*k, mnk_id = floor(mnk/329334390)) %>% filter(mnk_id %%2==1) -> dgemm2
dgemm2 %>% 
    group_by(type) %>% 
    mutate(mnk= mnk + ifelse(type=="M2/N0 (polynomial)",1.5E8,ifelse(type=="M2/N2 (polynomial + noise)",3E8,0)),
           duration = ifelse(type %in% c("M2/N0 (polynomial)","M2/N2 (polynomial + noise)"),duration/1.05,duration)) %>% # This is because this prediction uses the HPL correction
    ungroup() -> dgemm2
dgemm2 %>% tail(n=10)
dgemm2 %>% str()
#+end_src

#+RESULTS:
#+begin_example

# A tibble: 10 x 11
   func      m     n     k  node  core   cpu type        duration     mnk mnk_id
   <fct> <dbl> <dbl> <dbl> <int> <int> <int> <fct>          <dbl>   <dbl>  <dbl>
 1 dgemm   259  1215  1047     1     6     2 M2/N2 (pol…   0.0271  6.29e8      1
 2 dgemm   640  1160  9441     1    24     2 M2/N2 (pol…   0.492   7.31e9     21
 3 dgemm   547  3908  3279     1    24     2 M2/N2 (pol…   0.480   7.31e9     21
 4 dgemm   757  9002   440     1    10     2 M2/N2 (pol…   0.206   3.30e9      9
 5 dgemm  1841  2133  1615     1    26     2 M2/N2 (pol…   0.469   6.64e9     19
 6 dgemm  1442   912  2781     1    12     2 M2/N2 (pol…   0.253   3.96e9     11
 7 dgemm  4696   653  2717     1    28     2 M2/N2 (pol…   0.557   8.63e9     25
 8 dgemm  3136  9602   321     1     8     2 M2/N2 (pol…   0.638   9.97e9     29
 9 dgemm  3136   321  9602     1     8     2 M2/N2 (pol…   0.737   9.97e9     29
10 dgemm  1475   917  1719     1     8     2 M2/N2 (pol…   0.158   2.63e9      7

Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	2257 obs. of  11 variables:
 $ func    : Factor w/ 2 levels "6","dgemm": 2 2 2 2 2 2 2 2 2 2 ...
 $ m       : num  695 7359 1313 1887 547 ...
 $ n       : num  695 311 6716 987 3279 ...
 $ k       : num  695 441 642 1610 3908 ...
 $ node    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ core    : int  18 24 16 4 12 30 28 30 12 16 ...
 $ cpu     : int  2 2 2 2 2 2 2 2 2 2 ...
 $ type    : Factor w/ 4 levels "Reality","M1/N0 (linear)",..: 2 1 1 1 1 1 1 1 1 1 ...
 $ duration: num  0 0.0731 0.3878 0.2065 0.5059 ...
 $ mnk     : num  3.36e+08 1.01e+09 5.66e+09 3.00e+09 7.01e+09 ...
 $ mnk_id  : num  1 3 17 9 21 27 7 5 1 5 ...
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 300 :height 400 :session *R*
MSet1 <- c("#E41A1C", "#000000", "#377EB8", "#FF7F00", 
           "#FFFF33", "#A65628", "#F781BF", "#999999");
plot = ggplot(dgemm2, aes(x=mnk, y=duration,color=type)) + 
    geom_point(alpha=0.3,size=.3) + xlim(0,1.2E10)
    ## geom_point(aes(x=m*n*k+.5E8, y=prediction/1.05),alpha=0.3, color="blue") +
    ## geom_point(aes(x=m*n*k+1E8, y=pred_noise/1.05),alpha=0.3, color="green")
plot = plot + theme_bw() + ylab('Duration (s)') + 
        scale_x_continuous(name = 'M.N.K', breaks = (0:4)*3E9, limits=c(0,1.2E10)) 
plot = plot + annotate('text', x=0, y=0.75, hjust=0, vjust=0, size=2.5, fontface='italic', label='(Both M2 models are shifted to\n the right to improve readability)')
plot = plot + scale_color_manual(values=MSet1, guide=F) + #    scale_color_brewer(palette="Set1")
    theme(legend.position = c(1, 0), legend.justification=c(1, 0), legend.text=element_text(size=8), legend.box.background=element_rect(colour = "black"),
          panel.border=element_rect(colour = "black", fill=NA), legend.title=element_blank()) + #(size = 7, face = "italic")) +
    geom_smooth(data=dgemm2[dgemm2$type=="Reality",], size=.2,
                color="black", method='lm', se=F, fullrange=T) +
    guides(colour = guide_legend(override.aes = list(alpha = 1)))
ggsave(filename="img/prediction/modeling/kernels/dgemm_model_calib.png", plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figureTYBopG.png]]
***** HPL_dlatcpy
Scatter plot to show the time variability and how we model it.

#+begin_src R :results output :session *R2* :exports both
df = read.csv('data/prediction/modeling/kernels/trace_functions.csv')
print(unique(df$func))
# in this experiment, we got the nodes dahu-{1,...,8} and the ranks were mapped in the right order
df$node = 1 + df$rank %/% 32
df$cpu = 2*df$node + df$rank %% 2
df$mnk = df$m * df$n * df$k
df$mn = df$m * df$n
df$mk = df$m * df$k
df$nk = df$n * df$k
head(df)
#+end_src

#+RESULTS:
#+begin_example

[1] dtrsm         dgemm         HPL_dlatcpy   HPL_dlaswp03T HPL_dlaswp02N
[6] ion          
Levels: dgemm dtrsm HPL_dlaswp02N HPL_dlaswp03T HPL_dlatcpy ion

         func     m n  k   start        end    duration rank    mode node cpu
1       dtrsm     2 2 NA 0.01674 0.01678434 0.000044337    0 reality    1   2
2       dgemm 50046 2  2 0.01678 0.01698043 0.000200426    0 reality    1   2
3 HPL_dlatcpy     2 2 NA 0.01726 0.01726033 0.000000326    0 reality    1   2
4       dtrsm     4 4 NA 0.01726 0.01726144 0.000001438    0 reality    1   2
5       dgemm 50044 4  4 0.01727 0.01764994 0.000379944    0 reality    1   2
6       dtrsm     2 2 NA 0.01790 0.01790106 0.000001056    0 reality    1   2
     mnk     mn     mk nk
1     NA      4     NA NA
2 200184 100092 100092  4
3     NA      4     NA NA
4     NA     16     NA NA
5 800704 200176 200176 16
6     NA      4     NA NA
#+end_example

#+begin_src R :results output :session *R2* :exports both
func = df[(df$func == 'HPL_dlatcpy'),]
summary(lm(data=func, duration ~ (I(m*n):factor(cpu))+0))
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = duration ~ (I(m * n):factor(cpu)) + 0, data = func)

Residuals:
       Min         1Q     Median         3Q        Max 
-1.357e-03 -8.570e-06 -7.100e-07  4.000e-08  2.735e-03 

Coefficients:
                        Estimate Std. Error t value Pr(>|t|)    
I(m * n):factor(cpu)2  4.949e-09  1.012e-11 489.036   <2e-16 ***
I(m * n):factor(cpu)3  5.020e-09  9.889e-12 507.660   <2e-16 ***
I(m * n):factor(cpu)4  4.975e-09  1.011e-11 492.166   <2e-16 ***
I(m * n):factor(cpu)5  5.080e-09  1.037e-11 489.933   <2e-16 ***
I(m * n):factor(cpu)6  4.974e-09  1.019e-11 488.265   <2e-16 ***
I(m * n):factor(cpu)7  5.001e-09  1.027e-11 487.150   <2e-16 ***
I(m * n):factor(cpu)8  4.892e-09  1.013e-11 482.801   <2e-16 ***
I(m * n):factor(cpu)9  4.845e-09  1.051e-11 460.972   <2e-16 ***
I(m * n):factor(cpu)10 4.795e-09  1.022e-11 469.275   <2e-16 ***
I(m * n):factor(cpu)11 4.760e-09  1.046e-11 455.167   <2e-16 ***
I(m * n):factor(cpu)13 3.295e-09  3.937e-09   0.837    0.403    
---
codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 9.619e-05 on 21086 degrees of freedom
Multiple R-squared:  0.991,	Adjusted R-squared:  0.991 
F-statistic: 2.116e+05 on 11 and 21086 DF,  p-value: < 2.2e-16
#+end_example

OK. cpu 13, is the one with very few measurements and a weird
behavior. Let's get rid of it.

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R2*
library(ggplot2)
func = df[(df$func == 'HPL_dlatcpy') & (df$cpu!=13),]
func$duration = func$duration*1e3
func$mode_name = factor(ifelse(func$mode=='reality', 'Reality', 'M1/N2 (linear + noise)'))
fake_func = func[1,] # This is just to add a black legend for geom_smooth
fake_func$m = 0
fake_func$n = 0
fake_func$mode_name = factor("M1/N0 (linear)")
fake_func$duration = 0
func = rbind(fake_func, func)
func$mode_name = relevel(func$mode_name, 'Reality')

MSet1 <- c("#E41A1C", "#000000", "#377EB8", "#FF7F00",
           "#FFFF33", "#A65628", "#F781BF", "#999999");
plot = ggplot(func, aes(x=m*n, y=duration, color=mode_name)) +
    geom_smooth(data = func[func$mode == "reality",], aes(group=factor(cpu)), method='lm', 
                se=F, size=.2, color="blue", alpha=.2, fullrange=T) + 
    geom_point(alpha=.5,size=.5) + 
    geom_smooth(data = func[func$mode == "reality",], method='lm', se=F, fullrange=T, linetype=4, color="black")
plot = plot + theme_bw() + ylab('Duration (ms)') + xlab('M.N') +
    xlim(0,max(func$m*func$n)*1.1) +     
    scale_color_brewer(palette="Set1")  +
    scale_color_manual(values=MSet1) +
    theme(legend.position = c(0.3,0.85), legend.title=element_blank()) +
    guides(colour = guide_legend(override.aes = list(alpha = 1))) +
    theme(legend.position = c(1, 0), legend.justification=c(1, 0), legend.text=element_text(size=8), legend.box.background=element_rect(colour = "black"),
          panel.border = element_rect(colour = "black", fill=NA), legend.title=element_blank())

ggsave(filename='img/prediction/modeling/kernels/dlatcpy_model.png', plot=plot, width=7,height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figurep6JTMv.png]]
*** Generative model
**** Graphical representation of the model
#+header: :headers '("\\usepackage{graphicx}")
#+header: :exports results :results raw :file generative_model.pdf :tangle img/prediction/modeling/kernels/generative_model.tex
#+begin_src latex 
\documentclass{standalone}
\usepackage{color}
\usepackage{graphicx}
\DeclareGraphicsRule{.pdftex}{pdf}{*}{}%
%\graphicspath{{figures/}}
\begin{document}
\input{generative_model.pdftex_t}
\end{document}
#+end_src


#+begin_src shell :results output raw :exports both
base_dir=img/prediction/modeling/kernels/
filename=generative_model
base_path=${base_dir}/${filename}
fig2dev -L pdftex ${base_path}.fig > ${base_path}.pdftex
fig2dev -L pdftex_t -p generative_model.pdftex ${base_path}.fig > ${base_path}.pdftex_t
cd ${base_dir}; pdflatex ${filename}.tex >/dev/null 2>&1
echo file:${base_path}.pdf
#+end_src

#+RESULTS:
file:img/prediction/modeling/kernels//generative_model.pdf
*** Parameter distribution
The data for this plot comes from the non-regression repository, so it has a few
important characteristics to notice:
- the calibration was done with a random K (instead of fixed),
- the calibration was done with a single-threaded dgemm with one process per
  core (instead of a multi-threaded dgemm with one process per node)

The non-regression repository already has regression coefficients pre-computed,
but these regressions are done with the full polynomial. In the plot, we only
want the MNK term and the intercept. Thus, we had to re-do the regression.

According to the [[https://gitlab.in2p3.fr/cornebize/g5k_data_non_regression/-/blob/master/exp_changelog.org][changelog]]:
- the cooling issue started on [2019-09-01 Sun]
- we changed the protocol on [2019-10-18 Fri]
- the cooling issue was fixed on [2019-11-27 Wed]
- a BIOS upgrade happened on [2020-04-01 Wed]

So, the measures for when we had slow nodes were taken between the 2nd and the
3rd date, the measures when this problem was fixed were taken between the 3rd
and the 4th date.

The CSV files we use have been generated by:
- [[https://github.com/Ezibenroc/calibration_analysis/tree/8d69724a1181d814d97b1e207fb2b4e5c27de612/dahu/blas/extraction_and_regression.ipynb][this notebook]] for the slow-nodes dataframe,
- [[https://github.com/Ezibenroc/calibration_analysis/tree/4b520c94a9c15cfa0e5cdf53831740e294a02fdb/dahu/blas/extraction_and_regression.ipynb][this notebook]] for the non-slow-nodes dataframe.
Note that the only difference is in cell 3, which defines the time window in
which the data was collected on dahu.
**** Loading the data
#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(ggplot2)
library(patchwork)
library(Cairo)  # unicode characters in PDF output (need to install the Debian packages libxt-dev and libcairo2-dev)
library(scales) # function 'pretty_breaks' used in scale_{x,y}_continuous
#+end_src

#+RESULTS:

In the following code, we mark as special the data for CPU1 of dahu-15, as it is
a clear "outlier". This node was one of the nodes affected by the cooling issue,
however the problem was particularly strong on its CPU 1 with a *huge* effect on
the long-term temporal variability (resulting in a huge ellipse in the
alpha-gamma plot).

#+begin_src R :results output :session *R* :exports both
dfslow = read.csv("data/prediction/modeling/kernels/generative/dgemm_calibration_slownodes.csv")
str(dfslow)

tmpslow = dfslow %>%
    mutate(mode = ifelse(node %in% 13:16, "slow", "normal")) %>%
    mutate(mode = ifelse(node == 15 & cpu == 1, "very-slow", mode)) %>%
#    filter(node != 15 | cpu != 1) %>%
    mutate(node = interaction(node, cpu)) %>%
    dplyr::select(mnk, intercept, mnk_residual, node, mode)
colnames(tmpslow) = c("α", "β", "γ", "node", "mode")
tmpslow %>% head()
#+end_src

#+RESULTS:
#+begin_example

'data.frame':	1470 obs. of  14 variables:
 $ intercept          : num  2.45e-06 2.73e-06 2.75e-06 2.56e-06 2.66e-06 ...
 $ mnk                : num  7.69e-11 7.42e-11 7.56e-11 7.30e-11 7.57e-11 ...
 $ tvalue_mnk         : num  464 462 451 439 475 ...
 $ intercept_residual : num  4.36e-07 6.07e-07 3.61e-07 4.69e-07 4.05e-07 ...
 $ mnk_residual       : num  2.03e-12 2.23e-12 2.03e-12 1.98e-12 2.17e-12 ...
 $ tvalue_mnk_residual: num  13.7 15.8 13.6 13.5 15 ...
 $ avg_gflops         : num  25.9 26.8 26.3 27.2 26.3 ...
 $ function.          : Factor w/ 1 level "dgemm": 1 1 1 1 1 1 1 1 1 1 ...
 $ cluster            : Factor w/ 1 level "dahu": 1 1 1 1 1 1 1 1 1 1 ...
 $ node               : int  1 1 2 2 3 3 4 4 6 6 ...
 $ expfile_hash       : Factor w/ 24 levels "0b4efb0cfe0b952968222e3b80100dfd7827c354c362116773076509e7e7c189",..: 14 14 14 14 14 14 14 14 14 14 ...
 $ cpu                : int  0 1 0 1 0 1 0 1 0 1 ...
 $ jobid              : int  1889380 1889380 1889383 1889383 1889384 1889384 1889386 1889386 1889387 1889387 ...
 $ start_time         : int  1571386167 1571386167 1571386167 1571386167 1571386080 1571386080 1571386167 1571386167 1571386173 1571386173 ...

             α            β            γ node   mode
1 7.690653e-11 2.448521e-06 2.025623e-12  1.0 normal
2 7.423239e-11 2.726229e-06 2.233619e-12  1.1 normal
3 7.563839e-11 2.745021e-06 2.025757e-12  2.0 normal
4 7.300660e-11 2.561625e-06 1.980560e-12  2.1 normal
5 7.570254e-11 2.663667e-06 2.168812e-12  3.0 normal
6 7.273655e-11 2.470625e-06 1.984358e-12  3.1 normal
#+end_example

#+begin_src R :results output :session *R* :exports both
df = read.csv("data/prediction/modeling/kernels/generative/dgemm_calibration.csv")
str(df)

tmp = df %>%
    mutate(node = interaction(node, cpu)) %>%
    mutate(mode = "normal") %>%
    dplyr::select(mnk, intercept, mnk_residual, node, mode)
colnames(tmp) = c("α", "β", "γ", "node", "mode")
tmp %>% head()
#+end_src

#+RESULTS:
#+begin_example

'data.frame':	1456 obs. of  14 variables:
 $ intercept          : num  2.59e-06 2.35e-06 2.64e-06 2.33e-06 2.38e-06 ...
 $ mnk                : num  7.91e-11 7.49e-11 7.76e-11 7.40e-11 7.69e-11 ...
 $ tvalue_mnk         : num  183 177 176 171 178 ...
 $ intercept_residual : num  3.71e-07 4.10e-07 3.99e-07 3.37e-07 2.66e-07 ...
 $ mnk_residual       : num  2.80e-12 2.89e-12 2.75e-12 2.76e-12 2.83e-12 ...
 $ tvalue_mnk_residual: num  9.52 10.14 9.18 9.07 9.44 ...
 $ avg_gflops         : num  25.2 26.6 25.7 26.9 25.9 ...
 $ function.          : Factor w/ 1 level "dgemm": 1 1 1 1 1 1 1 1 1 1 ...
 $ cluster            : Factor w/ 1 level "dahu": 1 1 1 1 1 1 1 1 1 1 ...
 $ node               : int  1 1 2 2 3 3 4 4 5 5 ...
 $ expfile_hash       : Factor w/ 23 levels "07903fe17654a82bcd0b7bb463b7aa9531a0fa4c28cd63c3786e081306f3b8cc",..: 18 18 18 18 18 18 18 18 18 18 ...
 $ cpu                : int  0 1 0 1 0 1 0 1 0 1 ...
 $ jobid              : int  1895763 1895763 1895763 1895763 1895763 1895763 1895763 1895763 1895763 1895763 ...
 $ start_time         : int  1574848720 1574848720 1574848720 1574848720 1574848720 1574848720 1574848720 1574848720 1574848720 1574848720 ...

             α            β            γ node   mode
1 7.905501e-11 2.588708e-06 2.797882e-12  1.0 normal
2 7.494961e-11 2.354146e-06 2.891201e-12  1.1 normal
3 7.755325e-11 2.641479e-06 2.751326e-12  2.0 normal
4 7.403500e-11 2.326625e-06 2.762763e-12  2.1 normal
5 7.687820e-11 2.384417e-06 2.828755e-12  3.0 normal
6 7.329954e-11 2.327250e-06 2.710160e-12  3.1 normal
#+end_example

#+begin_src R :results output :session *R* :exports both
tmpslow %>%
    group_by(node) %>%
    summarize(nb=n()) %>%
    ungroup() %>%
    summarise(min_n=min(nb), med_n=median(nb), max_n=max(nb)) %>%
    as.data.frame()

tmp %>%
    group_by(node) %>%
    summarize(nb=n()) %>%
    ungroup() %>%
    summarise(min_n=min(nb), med_n=median(nb), max_n=max(nb)) %>%
    as.data.frame()
#+end_src

#+RESULTS:
: 
: `summarise()` ungrouping output (override with `.groups` argument)
:   min_n med_n max_n
: 1    16    24    24
: 
: `summarise()` ungrouping output (override with `.groups` argument)
:   min_n med_n max_n
: 1    18    23    23

**** Generating "synthetic data" (non-slow nodes)
#+begin_src R :results output :session *R* :exports both
dd = data.frame(tmp)
colnames(dd) = c("alpha", "beta", "gamma", "node", "mode")
#+end_src

#+RESULTS:

Computation of \mu and \mu_i
#+begin_src R :results output :session *R* :exports both
mu_i = dd %>%
    group_by(node) %>%
    summarize(alpha=mean(alpha), beta=mean(beta), gamma=mean(gamma)) %>%
    ungroup()
mu = mu_i %>%
    summarize(alpha=mean(alpha), beta=mean(beta), gamma=mean(gamma))
mu %>% as.data.frame()
#+end_src

#+RESULTS:
: 
: `summarise()` ungrouping output (override with `.groups` argument)
: 
:          alpha         beta       gamma
: 1 7.382283e-11 2.403732e-06 2.78301e-12

Computation of \sigma_S
#+begin_src R :results output :session *R* :exports both
sigma_S = cov(mu_i %>% dplyr::select(-node))
sigma_S
#+end_src

#+RESULTS:
: 
:              alpha         beta        gamma
: alpha 6.322087e-24 4.461153e-20 1.185202e-25
: beta  4.461153e-20 2.059323e-15 1.481189e-21
: gamma 1.185202e-25 1.481189e-21 6.804498e-27

Computation of \sigma_T as a mean of covariance matrices
#+begin_src R :results output :session *R* :exports both
nb_nodes = dd %>% pull(node) %>% unique() %>% length()
sigma_T_v1 = dd %>%
    group_by(node) %>%
    do(sigma=(cov(.[,1:3], .[,1:3]))) %>%
    ungroup() %>%
    pull(sigma) %>%
    Reduce("+", .) / nb_nodes
sigma_T_v1
#+end_src

#+RESULTS:
: 
:              alpha         beta        gamma
: alpha 3.368234e-25 5.903023e-21 3.031569e-25
: beta  5.903023e-21 2.315152e-14 7.342419e-21
: gamma 3.031569e-25 7.342419e-21 3.443662e-25

Computation of \sigma_T as the covariance matrices of centered data
#+begin_src R :results output :session *R* :exports both
sigma_T_v2 = dd %>%
    full_join(mu_i, by="node", suffix=c("", "_avg")) %>%
    mutate(alpha=alpha-alpha_avg, beta=beta-beta_avg, gamma=gamma-gamma_avg) %>%
    dplyr::select(alpha, beta, gamma) %>%
    cov()
sigma_T_v2
#+end_src

#+RESULTS:
: 
:              alpha         beta        gamma
: alpha 3.223913e-25 5.742764e-21 2.901382e-25
: beta  5.742764e-21 2.211188e-14 7.055505e-21
: gamma 2.901382e-25 7.055505e-21 3.296228e-25

Alright, both \sigma_T versions are pretty close, let's use the last one.

Now, we generate new data using the previously computed hyper-parameters.

#+begin_src R :results output :session *R* :exports both
library(MASS)  # mvrnorm function, to sample multivariate normal variables

generate_data <- function(avg_vec, spatial_matrix, temporal_matrix, nb_groups, nb_obs) {
    set.seed(42)
    averages = mvrnorm(n=nb_groups, mu=avg_vec, Sigma=spatial_matrix)
    return(generate_data2(averages, temporal_matrix, nb_obs))
}

generate_data2 <- function(averages, temporal_matrix, nb_obs) {
    set.seed(24)
    df = data.frame()
    for(i in (1:dim(averages)[1])) {
        observations = mvrnorm(n=nb_obs, mu=averages[i,], Sigma=temporal_matrix)
        df = rbind(df, data.frame(obs=observations, group=i, true_average=averages[i]))
    }
    return(df)
}

dfgen = generate_data(as.numeric(as.vector(mu)), sigma_S, sigma_T_v2, 16, 100)
colnames(dfgen) = c("α", "β", "γ", "group", "true_average")
dfgen$mode = "normal"
dfgen %>% head()
#+end_src

#+RESULTS:
: 
:              α            β            γ group true_average   mode
: 1 7.188021e-11 2.422691e-06 2.870758e-12     1 7.181616e-11 normal
: 2 7.320630e-11 2.261728e-06 4.440385e-12     1 7.181616e-11 normal
: 3 7.241638e-11 2.279120e-06 2.821549e-12     1 7.181616e-11 normal
: 4 7.195246e-11 2.428304e-06 2.796521e-12     1 7.181616e-11 normal
: 5 7.131970e-11 2.215501e-06 2.599286e-12     1 7.181616e-11 normal
: 6 7.140590e-11 2.301961e-06 2.290807e-12     1 7.181616e-11 normal
**** Generating "synthetic data" (slow nodes)
#+begin_src R :results output :session *R* :exports both
dd = data.frame(tmpslow %>% filter(mode != "very-slow"))
colnames(dd) = c("alpha", "beta", "gamma", "node", "mode")
#+end_src

#+RESULTS:

Computation of \mu and \mu_i
#+begin_src R :results output :session *R* :exports both
mu_i = dd %>%
    group_by(node, mode) %>%
    summarize(alpha=mean(alpha), beta=mean(beta), gamma=mean(gamma)) %>%
    ungroup()
mu = mu_i %>%
    group_by(mode) %>%
    summarize(alpha=mean(alpha), beta=mean(beta), gamma=mean(gamma))
mu %>% as.data.frame()
mu_normal = mu %>% filter(mode == "normal") %>% dplyr::select(-mode) %>% as.numeric()
mu_normal
mu_slow   = mu %>% filter(mode == "slow")   %>% dplyr::select(-mode) %>% as.numeric()
mu_slow
#+end_src

#+RESULTS:
#+begin_example

`summarise()` regrouping output by 'node' (override with `.groups` argument)

`summarise()` ungrouping output (override with `.groups` argument)

    mode        alpha         beta        gamma
1 normal 7.359773e-11 2.430287e-06 2.579149e-12
2   slow 8.297494e-11 2.557930e-06 4.435328e-12

[1] 7.359773e-11 2.430287e-06 2.579149e-12

[1] 8.297494e-11 2.557930e-06 4.435328e-12
#+end_example

Computation of \sigma_S
#+begin_src R :results output :session *R* :exports both
sigma_S_normal = cov(mu_i %>% filter(mode == "normal") %>% dplyr::select(-mode, -node))
sigma_S_normal
sigma_S_slow   = cov(mu_i %>% filter(mode == "slow")   %>% dplyr::select(-mode, -node))
sigma_S_slow
#+end_src

#+RESULTS:
#+begin_example

             alpha         beta        gamma
alpha 6.503416e-24 5.365646e-20 1.019381e-25
beta  5.365646e-20 2.320419e-15 1.790849e-21
gamma 1.019381e-25 1.790849e-21 7.910278e-27

             alpha         beta        gamma
alpha 3.125799e-23 4.826488e-19 1.460527e-24
beta  4.826488e-19 8.457385e-15 2.515249e-20
gamma 1.460527e-24 2.515249e-20 2.401920e-25
#+end_example

Computation of \sigma_T as the covariance matrices of centered data
#+begin_src R :results output :session *R* :exports both
dd_extended = dd %>%
    full_join(mu_i, by="node", suffix=c("", "_avg")) %>%
    mutate(alpha=alpha-alpha_avg, beta=beta-beta_avg, gamma=gamma-gamma_avg)
sigma_T_v2_normal = dd_extended %>%
    filter(mode == "normal") %>%
    dplyr::select(alpha, beta, gamma) %>%
    cov()
sigma_T_v2_slow = dd_extended %>%
    filter(mode == "slow") %>%
    dplyr::select(alpha, beta, gamma) %>%
    cov()
sigma_T_v2_normal
sigma_T_v2_slow
#+end_src

#+RESULTS:
#+begin_example

              alpha          beta         gamma
alpha  1.978240e-25 -6.298417e-21  1.150207e-25
beta  -6.298417e-21  3.596563e-14 -9.379901e-21
gamma  1.150207e-25 -9.379901e-21  1.430723e-25

             alpha          beta         gamma
alpha 2.462951e-24  1.162079e-20  1.272258e-24
beta  1.162079e-20  4.324072e-14 -7.566107e-21
gamma 1.272258e-24 -7.566107e-21  9.974587e-25
#+end_example

Now, we generate new data using the previously computed hyper-parameters.

#+begin_src R :results output :session *R* :exports both
library(MASS)  # mvrnorm function, to sample multivariate normal variables

generate_data <- function(avg_vec, spatial_matrix, temporal_matrix, nb_groups, nb_obs) {
    set.seed(42)
    averages = mvrnorm(n=nb_groups, mu=avg_vec, Sigma=spatial_matrix)
    return(generate_data2(averages, temporal_matrix, nb_obs))
}

generate_data2 <- function(averages, temporal_matrix, nb_obs) {
    set.seed(24)
    df = data.frame()
    for(i in (1:dim(averages)[1])) {
        observations = mvrnorm(n=nb_obs, mu=averages[i,], Sigma=temporal_matrix)
        df = rbind(df, data.frame(obs=observations, group=i, true_average=averages[i]))
    }
    return(df)
}

dfgen_normal = generate_data(mu_normal, sigma_S_normal, sigma_T_v2_normal, 14, 100) %>%
    mutate(mode = "normal")
dfgen_slow = generate_data(mu_slow,   sigma_S_slow,   sigma_T_v2_slow,    2, 100) %>%
    mutate(mode = "slow") %>%
    mutate(group = group + (dfgen_normal %>% pull(group) %>% max() + 1))
dfgen_slow = rbind(dfgen_normal, dfgen_slow)
colnames(dfgen_slow) = c("α", "β", "γ", "group", "true_average", "mode")
dfgen_slow %>% head()
#+end_src

#+RESULTS:
: 
:              α            β            γ group true_average   mode
: 1 7.178414e-11 2.467771e-06 2.556473e-12     1 7.176439e-11 normal
: 2 7.308078e-11 2.262486e-06 3.341421e-12     1 7.176439e-11 normal
: 3 7.172739e-11 2.284667e-06 3.161618e-12     1 7.176439e-11 normal
: 4 7.170358e-11 2.474930e-06 2.651174e-12     1 7.176439e-11 normal
: 5 7.172055e-11 2.203530e-06 2.209385e-12     1 7.176439e-11 normal
: 6 7.138794e-11 2.313797e-06 2.365847e-12     1 7.176439e-11 normal
**** Plot function & bounds
To have the same scale on all plots, we compute global bounds.
#+begin_src R :results output :session *R* :exports both
plot_bounds = bind_rows(tmp, tmpslow, dfgen, dfgen_slow) %>% dplyr::select(-node, -group, -true_average, -mode) %>% summarise_all(funs(min, max))
plot_bounds
#+end_src

#+RESULTS:
: 
:          α_min        β_min        γ_min        α_max        β_max        γ_max
: 1 6.549004e-11 1.733146e-06 9.452878e-13 9.310914e-11 3.361271e-06 1.987595e-11

#+begin_src R :results output :session *R* :exports both
plot_bivariate <- function(df, x_col, y_col, group_col, mode_col, bounds, color_mode=F) {
    color_col = group_col
    if(color_mode) {
        color_col = mode_col
    }
    centers = df %>%
        group_by_(group_col, mode_col) %>%
        summarise_all(funs(mean))

    density_plot = ggplot() +
        aes_string(color=color_col, group=group_col) +
# Uncomment the following line to have a density plot for the centers  
#    geom_density(data=centers, aes_string(group=mode_col), size=2, color="black") +
        geom_density(data=df) +
        theme_void()

    if(color_mode){
        density_plot = density_plot + scale_color_brewer(palette="Dark2", type="qual")
    }

    plot_top = density_plot +
        aes_string(x=x_col) +
        xlim(bounds[[paste(x_col, "_min", sep="")]], bounds[[paste(x_col, "_max", sep="")]])
    plot_right = density_plot +
        aes_string(x=y_col) +
        xlim(bounds[[paste(y_col, "_min", sep="")]], bounds[[paste(y_col, "_max", sep="")]]) +
        coord_flip()

    text = df %>%
        group_by_(group_col) %>%
        summarise_all(funs(mean))

    scatter_plot = ggplot(df) +
        aes_string(x=x_col, y=y_col, color=color_col, group=group_col) +
        geom_point(alpha=0.3) +
        stat_ellipse() +
# Uncomment the following line to put labels on the ellipses
#        geom_label(data=text, aes_string(label=group_col)) +
        theme_minimal() +
        scale_x_continuous(breaks=pretty_breaks(n=3), limits=c(bounds[[paste(x_col, "_min", sep="")]], bounds[[paste(x_col, "_max", sep="")]])) +
        scale_y_continuous(breaks=pretty_breaks(n=3), limits=c(bounds[[paste(y_col, "_min", sep="")]], bounds[[paste(y_col, "_max", sep="")]])) +
        theme(text=element_text(size=24))

    if(color_mode){
        scatter_plot = scatter_plot + scale_color_brewer(palette="Dark2", type="qual")
    }

    
    return(
        plot_top + plot_spacer() + scatter_plot + plot_right +
        plot_layout(widths = c(4, 1), heights = c(1, 4)) &
        theme(legend.position='none')
    )
}
#+end_src

#+RESULTS:

**** Calibration data (slow nodes)

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 600 :session *R*
p = plot_bivariate(tmpslow, "α", "β", "node", "mode", plot_bounds, color_mode=T)
ggsave("img/prediction/modeling/kernels/whatif_calibration_slownodes_1.pdf", plot=p, device=cairo_pdf, width = 10, height = 6)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figurerMSo14.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 600 :session *R*
p = plot_bivariate(tmpslow, "α", "γ", "node", "mode", plot_bounds, color_mode=T)
ggsave("img/prediction/modeling/kernels/whatif_calibration_slownodes_2.pdf", plot=p, device=cairo_pdf, width = 10, height = 6)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figure0zf4ko.png]]
**** Calibration data (non-slow nodes)
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 600 :session *R*
p = plot_bivariate(tmp, "α", "β", "node", "mode", plot_bounds)
ggsave("img/prediction/modeling/kernels/whatif_calibration_1.pdf", plot=p, device=cairo_pdf, width = 10, height = 6)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figureXFMcFT.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 600 :session *R*
p = plot_bivariate(tmp, "α", "γ", "node", "mode", plot_bounds)
ggsave("img/prediction/modeling/kernels/whatif_calibration_2.pdf", plot=p, device=cairo_pdf, width = 10, height = 6)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figureb0XeZP.png]]

**** Synthetic model
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 600 :session *R*
p = plot_bivariate(dfgen, "α", "β", "factor(group)", "mode", plot_bounds)
ggsave("img/prediction/modeling/kernels/whatif_model_1.pdf", plot=p, device=cairo_pdf, width = 10, height = 6)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figureKWNuAr.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 600 :session *R*
p = plot_bivariate(dfgen, "α", "γ", "factor(group)", "mode", plot_bounds)
ggsave("img/prediction/modeling/kernels/whatif_model_2.pdf", plot=p, device=cairo_pdf, width = 10, height = 6)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figureKDDaoO.png]]

**** Synthetic model (slow nodes)
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 600 :session *R*
p = plot_bivariate(dfgen_slow, "α", "β", "factor(group)", "mode", plot_bounds, color_mode=T)
ggsave("img/prediction/modeling/kernels/whatif_model_slow_1.pdf", plot=p, device=cairo_pdf, width = 10, height = 6)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figure94lyLb.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 600 :session *R*
p = plot_bivariate(dfgen_slow, "α", "γ", "factor(group)", "mode", plot_bounds, color_mode=T)
ggsave("img/prediction/modeling/kernels/whatif_model_slow_2.pdf", plot=p, device=cairo_pdf, width = 10, height = 6)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figuredDqKzi.png]]

*** Network plots                                                :noexport:
**** Downloading the CSV
#+begin_src sh :results output :exports both
mkdir -p data/prediction/modeling/network
cd data/prediction/modeling/network
mkdir -p stampede && cd stampede
wget -c https://gitlab.inria.fr/simgrid/platform-calibration/raw/master/data/stampede_17_06_01-17:14/calibration/testplatform_PingPong.csv -O pingpong.csv
wget -c https://gitlab.inria.fr/simgrid/platform-calibration/raw/master/data/stampede_17_06_01-17:14/calibration/testplatform_Recv.csv -O recv.csv
cd ..
mkdir -p dahu && cd dahu
wget -c https://github.com/Ezibenroc/mpi_calibration/raw/master/dahu/mpi/grenoble_2018-08-29_1808878.zip -O archive.zip
unzip -p archive.zip exp/exp_PingPong.csv > pingpong.csv
unzip -p archive.zip exp/exp_Recv.csv > recv.csv
#+end_src

#+RESULTS:

**** Drawing the regression plots
#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(dplyr)
library(gridExtra)

read_csv <- function(filename) {
    df = read.csv(filename, header=F)
    colnames(df) = c('func', 'msg_size', 'start', 'duration')
#    df = df[sample(nrow(df), 1000), ]  # take only some points, for quick prototyping of the plot
    return(df)
}

draw_reg <- function(df, calibration_df) {
    platforms = unique(calibration_df$platform)

    # Computing the groups
    df$group = 0
    for(plat in platforms) {
        i = 1
        for(bp in calibration_df[calibration_df$platform == plat,]$breakpoint) {
            df[df$platform==plat & df$msg_size > bp,]$group = i
            i = i+1
        }
    }
    df$group = as.factor(df$group)

    # Basic plot
    plot = ggplot(df, aes(x=msg_size, y=duration, color=group)) + geom_point(size=.5, alpha=0.1)
    plot = plot + scale_x_log10() + scale_y_log10() + theme_bw() + scale_color_discrete(guide=F)
    plot = plot + xlab('Message size (bytes)')  + ylab('Duration (seconds)') # + ylab(paste(unique(df$func), 'duration\n (seconds)'))

    # Computing and plotting the regressions
    df$pred = -1
    for(plat in unique(df$platform)) {
        for(grp in unique(df$group)) {
            for(func in unique(df$func)) {
                index = df$group == grp & df$func == func & df$platform == plat
                reg = lm(duration~msg_size, df[index,])
                df[index,]$pred = predict(reg, df[index,])
            }
        }
    }
    plot = plot + geom_line(aes(y=pred, group=group), data=df, color='black')

    # Plotting the breakpoints
    breakpoints_recv = data.frame(calibration_df)
    breakpoints_recv$func = 'MPI_Recv'
    breakpoints_send = data.frame(calibration_df)
    breakpoints_send$func = 'MPI_Send'
    plot = plot + geom_vline(aes(xintercept=breakpoint), data=rbind(breakpoints_recv, breakpoints_send), linetype='dashed')

    # Plotting the labels
    txt = data.frame(func=rep(unique(df$func), 2), msg_size=rep(c(1), 4), duration=rep(c(5e-5), 4), platform=sort(rep(unique(df$platform), 2)))
    txt$label = paste(toupper(txt$platform), txt$func, sep='\n')
    plot = plot + geom_label(aes(label=label), color='black', data=txt, size=4, hjust=0)

    # Wrapping
    plot = plot + facet_wrap(c('func', 'platform'), nrow=2) + theme(strip.background = element_blank(), strip.text.x = element_blank())
    return(plot)
}

draw_mpi_reg <- function(calibration_df) {
    df = data.frame()
    for(platform in unique(calibration_df$platform)) {
        pingpong_file = paste('data/prediction/modeling/network', platform, 'pingpong.csv', sep='/')
        recv_file     = paste('data/prediction/modeling/network', platform, 'recv.csv',     sep='/')
        df_pingpong = read_csv(pingpong_file)
        df_send = df_pingpong %>% filter(func == 'MPI_Send')
        df_recv = read_csv(recv_file)
        tmp = rbind(df_send, df_recv)
        tmp$platform = platform
        df = rbind(df, tmp)
    }
    return(draw_reg(df, calibration_df))
}
#+end_src

#+RESULTS:

#+NAME: table_mpi_calibration
| platform | breakpoint |
|----------+------------|
| dahu     |       8133 |
| dahu     |      15831 |
| dahu     |      33956 |
| dahu     |      64000 |
| stampede |        150 |
| stampede |       5000 |
| stampede |      17420 |
| stampede |     110000 |

#+begin_src R :results output graphics :var calibrations=table_mpi_calibration :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = draw_mpi_reg(calibrations)
ggsave(filename='img/prediction/emulating/mpi_calibration.png', plot=plot, width=6,height=4, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Kejzdq/figure8FQI9S.png]]
*** Pycewise durations
**** Running the experiment
This experiment was done with the following script (file named
=script_pycewise_duration.py=):
#+begin_src python :results output :session *python* :exports both
import pycewise
import random
import pandas
import numpy
import itertools
import time

print(pycewise.__version__)
print(pycewise.__git_version__)

NB_RUNS = 10
SIZES = [100, 200, 300, 400, 500]


def model(x):
    if x <= 1.450e+04:
        y = 1.739e-11*x + 4.181e-08
    elif x <= 9.925e+05:
        y = 5.457e-11*x
    elif x <= 5.630e+06:
        y = 8.929e-11*x
    else:
        y = 1.935e-10*x + 1.294e-05
    return y


def generate_model(func, N):
    x = 10**numpy.random.uniform(1, 9, N)
    y = numpy.array(list(map(func, x)))
    return pandas.DataFrame({'x': x, 'y': y})


def run_test(N, seed):
    numpy.random.seed(seed)
    random.seed(seed)
    df = {}
    base_df = generate_model(model, N)
    df['no noise'] = base_df
    df['homoscedastic'] = base_df.copy()
    df['homoscedastic']['y'] += numpy.random.normal(0, 5e-9, len(base_df))
    df['heteroscedastic'] = base_df.copy()
    df['heteroscedastic']['y'] += numpy.random.normal(0, 2e-12, len(base_df)) * base_df['x']
    exp = list(df.items())
    exp = list(itertools.product(exp, ['BIC', 'weighted', 'log']))
    random.shuffle(exp)
    result = []
    for (name, data), mode in exp:
        start = time.time()
        try:
            reg = pycewise.compute_regression(data['x'], data['y'], mode=mode).auto_simplify()
        except Exception:
            raise Exception(f'Problem with N={N} and seed={seed} and mode={mode} and noise={name}')
        duration = time.time() - start
        result.append({'duration': duration, 'timestamp': start, 'mode': mode, 'noise': name,
                        'nb_breaks': len(reg.breakpoints), 'seed': seed, 'size': N})
    return pandas.DataFrame(result)

start = time.time()
exp = list(itertools.product(SIZES, range(NB_RUNS)))
random.seed(42)
random.shuffle(exp)

dataframes = []
for N, seed in exp:
    dataframes.append(run_test(N, seed))
dataframes = pandas.concat(dataframes)

dataframes.to_csv('pycewise_durations.csv', index=False)
print(f'Terminated in {time.time()-start:.2f} seconds')
#+end_src

Then, the script has been copied to G5K and a job submitted:
#+begin_src sh :results output :exports both
scp script_pycewise_duration.py 'grenoble.g5k:~'
ssh grenoble.g5k 'oarsub -l "walltime=02:00:00" -n "pycewise" "sudo-g5k pip3 install pycewise numpy pandas && python3 script_pycewise_duration.py"'
#+end_src

Output of the job (that was scheduled on node dahu-28):
#+begin_src sh :results output :exports both
ssh grenoble.g5k 'cat OAR*pycewise*out'
#+end_src

#+RESULTS:
: Collecting pycewise
:   Downloading https://files.pythonhosted.org/packages/5e/6b/3563ee6f7c28ac490d2f936b557bc8a2e10aff1ae57d81e249236a1ef35a/pycewise-0.1.2-py3-none-any.whl
: Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.16.2)
: Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (0.23.3+dfsg)
: Installing collected packages: pycewise
: Successfully installed pycewise-0.1.2
: 0.1.2
: 27f261d5aada4d0bce84394bfe29d3cd3bc89d46
: Terminated in 1602.16 seconds

Finally, I downloaded the file on my laptop:
#+begin_src sh :results output :exports both
scp 'grenoble.g5k:*pycewise*.csv' .
#+end_src
**** Drawing the plot
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
library(dplyr)
library(ggplot2)
library(ggbeeswarm)
library(cowplot)

plot = read.csv("data/prediction/modeling/network/pycewise/pycewise_durations.csv") %>%
    mutate(noise=factor(noise, levels=c("no noise", "homoscedastic", "heteroscedastic"))) %>%
    mutate(mode=ifelse(mode=="BIC", "ordinary", as.character(mode))) %>%
    mutate(mode=factor(mode, levels=c("ordinary", "weighted", "log"))) %>%
    ggplot() +
        aes(x=factor(size), y=duration, color=noise) +
        geom_quasirandom(dodge.width=0.5, key_glyph=rectangle_key_glyph(fill=color)) +
        facet_wrap('mode')+#, scales='free') +
        expand_limits(y=0) +
        theme_bw() +
        scale_color_brewer(palette="Dark2") +
        xlab("Number of observations") +
        ylab("Duration of the regression (seconds)") +
        theme(legend.position=c(0.11, 0.75), legend.background=element_rect(color="black"), legend.title=element_blank())

ggsave(filename='img/prediction/modeling/network/pycewise_duration.pdf', plot=plot, width=7, height=3)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Ot77lg/figurelmTCpr.png]]
** Validation
*** Gantt Chart
**** Downloading the CSV
These CSV have been generated with this [[https://github.com/Ezibenroc/calibration_analysis/blob/3933006b4bfb9b3ce5a8d0758258f6b1fd7957c2/dahu/hpl/hpl_trace.ipynb][notebook]] (the dump to the CSV files
happens in the function paper_plot, cell 14, that is called in cells 15 and 16).
The notebook uses the ZIP archives from this [[https://github.com/Ezibenroc/calibration_analysis/tree/0d4f39b8391597c93861482621b98f814272bdb4/dahu/smpi_hpl/paper_sc19/traces/2][directory]].

#+begin_src sh :results output :exports both
cd data/prediction/validation/traces
wget -c https://github.com/Ezibenroc/calibration_analysis/raw/master/dahu/smpi_hpl/paper_sc19/traces/2/trace_reality.csv
wget -c https://github.com/Ezibenroc/calibration_analysis/raw/master/dahu/smpi_hpl/paper_sc19/traces/2/trace_simulation_deterministic-CPU_linear-DGEMM_deterministic-network.csv
wget -c https://github.com/Ezibenroc/calibration_analysis/raw/master/dahu/smpi_hpl/paper_sc19/traces/2/trace_simulation_deterministic-CPU_linear-DGEMM_stochastic-network.csv
wget -c https://github.com/Ezibenroc/calibration_analysis/raw/master/dahu/smpi_hpl/paper_sc19/traces/2/trace_simulation_stochastic-CPU_polynomial-DGEMM_deterministic-network.csv
wget -c https://github.com/Ezibenroc/calibration_analysis/raw/master/dahu/smpi_hpl/paper_sc19/traces/2/trace_simulation_stochastic-CPU_polynomial-DGEMM_stochastic-network.csv
wc -l *csv
du -sh *csv
#+end_src

#+RESULTS:
#+begin_example
   222338 trace_reality.csv
   218227 trace_simulation_deterministic-CPU_linear-DGEMM_deterministic-network.csv
   219151 trace_simulation_deterministic-CPU_linear-DGEMM_stochastic-network.csv
   222973 trace_simulation_stochastic-CPU_polynomial-DGEMM_deterministic-network.csv
   225352 trace_simulation_stochastic-CPU_polynomial-DGEMM_stochastic-network.csv
  1108041 total
23M	trace_reality.csv
37M	trace_simulation_deterministic-CPU_linear-DGEMM_deterministic-network.csv
36M	trace_simulation_deterministic-CPU_linear-DGEMM_stochastic-network.csv
37M	trace_simulation_stochastic-CPU_polynomial-DGEMM_deterministic-network.csv
37M	trace_simulation_stochastic-CPU_polynomial-DGEMM_stochastic-network.csv
#+end_example
**** Drawing the Gantt charts
#+NAME: init_gantt
#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(dplyr)
# Levels: MPI_Recv MPI_Send dgemm dtrsm other
colours <- c("#5050AA", "#AA2020", "#FFFF88", "#FF00FF", "#FF0000")
line_color='black'

draw_gantt <- function(csv_name, xmax= 10, xstep = 2, label_line=F) {
    df = read.csv(csv_name)
    df$func = factor(df$func, levels=c('MPI_Recv', 'MPI_Send', 'dgemm', 'dtrsm', 'other'))
    dfend = df %>% group_by(rank) %>% filter(func=="dgemm") %>% summarize(end=max(end)) %>% select(end) %>% ungroup()  %>% summarize(end=mean(end))
    myend <<- dfend$end;
    xbreaks=seq(from=0, to=xmax, by=xstep);
    gc = ggplot(df) + geom_rect(aes(xmin=start, xmax=end, ymin=rank, ymax=rank+1, fill=func)) + theme_bw() +
        scale_fill_manual(values=colours,  guide=F) +  # scale_fill_brewer(type='qualitative', palette='Set1',) 
        geom_vline(data=data.frame(x=xbreaks),aes(xintercept=x), linetype=2) +
        geom_vline(data=dfend,aes(xintercept=end),color=line_color, size=2) +
        xlim(0,xmax) +
        theme(text=element_text(size=25), axis.title.x=element_blank(), axis.title.y=element_blank()) +
        scale_x_continuous(breaks=xbreaks);
    if(label_line) {
        y = mean(df$rank)*1.2
        xt = myend*1.3
        yt = y*0.7
        gc = gc + annotate("segment", x=myend, xend=xt, y=y, yend=yt, color=line_color, size=2)
        gc = gc + annotate("label", label='Average completion\nof iteration 5', size=8, x=xt, y=yt, colour=line_color)
    }
    return(gc)
}
#+end_src

#+RESULTS: init_gantt

#+RESULTS:

#+begin_src R :results output graphics :file img/prediction/validation/traces/gantt_reality.png :exports both :width 600 :height 400 :session *R* :var dep=init_gantt
draw_gantt("data/prediction/validation/traces/trace_reality.csv", label_line=T)
#+end_src

#+RESULTS:
[[file:img/prediction/validation/traces/gantt_reality.png]]

#+begin_src R :results output :session *R* :exports both :var dep=init_gantt
file_list=c("trace_reality", 
            "trace_simulation_deterministic-CPU_linear-DGEMM_deterministic-network", 
            "trace_simulation_deterministic-CPU_linear-DGEMM_stochastic-network",
            "trace_simulation_stochastic-CPU_polynomial-DGEMM_deterministic-network",
            "trace_simulation_stochastic-CPU_polynomial-DGEMM_stochastic-network",
            "trace_simulation_stochastic-CPU_polynomial-DGEMM_stochastic-network");
for(i in file_list) {
    png(filename=paste0("img/prediction/validation/traces/",gsub("^trace_","gantt_",i),".png"),width=900,height=300); 
    ll=F;
    if(i=="trace_simulation_deterministic-CPU_linear-DGEMM_deterministic-network") {
        ll=T;
    }
    gc=draw_gantt(paste0("data/prediction/validation/traces/",i,".csv"), label_line=ll);
    plot(gc);
    dev.off();
}
#+end_src

#+RESULTS:
#+begin_example

`summarise()` ungrouping output (override with `.groups` argument)
Scale for 'x' is already present. Adding another scale for 'x', which will
replace the existing scale.
`summarise()` ungrouping output (override with `.groups` argument)
Scale for 'x' is already present. Adding another scale for 'x', which will
replace the existing scale.
`summarise()` ungrouping output (override with `.groups` argument)
Scale for 'x' is already present. Adding another scale for 'x', which will
replace the existing scale.
`summarise()` ungrouping output (override with `.groups` argument)
Scale for 'x' is already present. Adding another scale for 'x', which will
replace the existing scale.
`summarise()` ungrouping output (override with `.groups` argument)
Scale for 'x' is already present. Adding another scale for 'x', which will
replace the existing scale.
`summarise()` ungrouping output (override with `.groups` argument)
Scale for 'x' is already present. Adding another scale for 'x', which will
replace the existing scale.
#+end_example

*** Matrix size
For this plot, the CSV files are small, so they are stored directly in the
repository, no need to download them. They have been dumped by [[https://github.com/Ezibenroc/calibration_analysis/blob/5cc53b7b1bdaa8746e6ca8822a04c9259571d851/dahu/smpi_hpl/hpl_simulation.ipynb][this notebook]]
(cell 2).

The zip archives for these files are:
- Real executions of October: [[https://github.com/Ezibenroc/calibration_analysis/blob/master/dahu/hpl/grenoble_2018-10-10_1811810.zip][this]] and [[https://github.com/Ezibenroc/calibration_analysis/blob/master/dahu/hpl/grenoble_2018-10-10_1811847.zip][this]] archives.
- Real executions of March: [[https://github.com/Ezibenroc/calibration_analysis/blob/master/dahu/hpl/grenoble_2019-03-15_1855885.zip][this]] and [[https://github.com/Ezibenroc/calibration_analysis/blob/master/dahu/hpl/grenoble_2019-03-18_1856059.zip][this]] archives.
- Simulations with the homogeneous dgemm model: [[https://github.com/Ezibenroc/calibration_analysis/tree/master/dahu/smpi_hpl/paper_sc19/performance/2][these archives]].
- Simulation with the heterogeneous dgemm model: [[https://github.com/Ezibenroc/calibration_analysis/tree/master/dahu/smpi_hpl/paper_sc19/performance/11][these archives]].
- Simulation with the heterogeneous dgemm model but simple models for the other parts: [[https://github.com/Ezibenroc/calibration_analysis/tree/master/dahu/smpi_hpl/paper_sc19/performance/12][these archives]].
**** Drawing the plot
Here, we use tikzDevice to generate Tikz code with ggplot. More information on
this [[https://matthew-parker.rbind.io/post/2018-08-21-easy-latex-titles/][tutorial]].

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(tikzDevice)
library(dplyr)
options(warn=-1)
dfreal = read.csv('data/prediction/validation/matrix_size/hpl_reality.csv')
dfreal = dfreal %>% filter(date == '2019-03')
dfsim = bind_rows(
    read.csv('data/prediction/validation/matrix_size/hpl_simulation_heterogeneous.csv'),
    read.csv('data/prediction/validation/matrix_size/hpl_simulation_homogeneous.csv')
)
dfsim$model = gsub("_het", "het", dfsim$model)  # tikzdevice fails if there are underscores in the node names...
head(dfreal)
head(dfsim)
#+end_src

#+RESULTS:
#+begin_example

  matrix_size gflops    time    mode    date
1      175000  22610  158.00 reality 2019-03
2      500000  22330 3732.41 reality 2019-03
3       50000  12780    6.52 reality 2019-03
4      300000  21140  851.46 reality 2019-03
5      400000  21660 1969.91 reality 2019-03
6      150000  19910  113.02 reality 2019-03

  matrix_size gflops    time       mode stochastic_cpu stochastic_network
1      100000  18870   35.33 simulation           True              False
2      200000  21390  249.29 simulation           True              False
3      400000  22690 1880.62 simulation           True              False
4       75000  15930   17.65 simulation           True               True
5      175000  19690  181.49 simulation           True               True
6      350000  21470 1331.50 simulation           True               True
  polynomial_dgemm heterogeneous_dgemm model simulation_time memory_consumption
1            False                True B1het         3227.07         2270136000
2            False                True B1het         6316.27         4134760000
3            False                True B1het        14848.10         7813704000
4             True                True D2het         1522.91         1703044000
5             True                True D2het         4094.11         3641888000
6             True                True D2het        10207.20         7549632000
#+end_example

Keeping only a subset of the models, to have a readable plot.
#+begin_src R :results output :session *R* :exports both
dfsim = dfsim %>% filter(model %in% c('A1', 'A2', 'A1het', 'C1het', 'D1het', 'D2het'))
#+end_src

#+RESULTS:

Removing the two outliers in the real executions. These two points are
"abnormaly fast", each of them was the first to be made in the sequence of runs
(the machines were not overheating yet).
#+begin_src R :results output :session *R* :exports both
print(length(dfreal$gflops))
dfreal = dfreal %>% filter(matrix_size != 175000 | gflops < 22000)
print(length(dfreal$gflops))
#+end_src

#+RESULTS:
: [1] 112
: 
: [1] 110

For information, the notations are:
#+BEGIN_SRC latex
\newcommand{\model}[2][]{\ensuremath{\mathcal{M}_{#1}\ifthenelse{\equal{#2}{}}{}{\!-\!}{#2}}\xspace}
\newcommand{\modelp}[2][]{\ensuremath{\mathcal{M'}_{#1}\!\ifthenelse{\equal{#2}{}}{}{\!-\!}{#2}}\xspace}
\newcommand{\noise}[2][]{\ensuremath{\mathcal{N}_{#1}\ifthenelse{\equal{#2}{}}{}{\!-\!}{#2}}\xspace}
\newcommand{\noisep}[2][]{\ensuremath{\mathcal{N'}_{#1}\!\ifthenelse{\equal{#2}{}}{}{\!-\!}{#2}}\xspace}
#+END_SRC

Let's compute the error done by each model for each matrix size. This is a
relative error (a percentage), computed with (perf_simu-perf_real)/perf_real.

#+begin_src R :results output :session *R* :exports both
library(stringr)
tmp = dfreal %>% group_by(matrix_size) %>% summarize(gflops=mean(gflops))
tmp = inner_join(dfsim, tmp, by='matrix_size', suffix=c('_simulation', '_reality'))
tmp$error = (tmp$gflops_simulation - tmp$gflops_reality)/tmp$gflops_reality

pretty_percent = function(value) {
    value = round(value*100)
    if(value > 0) {
        sign='+'
    }
    else {
        sign='-'
        value = -value
    }
    return(paste(sign, value, '\\%', sep=''))
}

tmp$pretty_error = sapply(tmp$error, pretty_percent)
head(tmp)
#+end_src

#+RESULTS:
#+begin_example

`summarise()` ungrouping output (override with `.groups` argument)

  matrix_size gflops_simulation    time       mode stochastic_cpu
1       75000             15930   17.65 simulation           True
2      175000             19690  181.49 simulation           True
3      350000             21470 1331.50 simulation           True
4       50000             13900    5.99 simulation          False
5      150000             20510  109.73 simulation          False
6      300000             23150  777.47 simulation          False
  stochastic_network polynomial_dgemm heterogeneous_dgemm model simulation_time
1               True             True                True D2het         1522.91
2               True             True                True D2het         4094.11
3               True             True                True D2het        10207.20
4              False            False                True A1het         1512.69
5              False            False                True A1het         4960.00
6              False            False                True A1het        10003.40
  memory_consumption gflops_reality        error pretty_error
1         1703044000       16588.75 -0.039710647        -4\\%
2         3641888000       19803.33 -0.005722942        -1\\%
3         7549632000       21765.00 -0.013553871        -1\\%
4         1159164000       12797.50  0.086149639        +9\\%
5         3436460000       19403.75  0.057012175        +6\\%
6         7782212000       21491.25  0.077182574        +8\\%
#+end_example


Then, we can generate the plot.

#+begin_src R :results output :session *R* :exports both
tikzDevice::tikz(file = 'img/prediction/validation/matrix_size/validation_performance.tex', width = 5, height = 3)
#tikzDevice::tikz(file = '/tmp/plot.tex', standAlone=T, width = 5, height = 3)

model_ <- function(letter, het, prime, nb) {
    if(prime) {
        letter = paste(letter, "'", sep='')
    }
    result = paste('\\mathcal{', letter, '}', sep='')
    if(het) {
        result = paste(result, '_{H}', sep='')
    }
    return(paste(result, '\\!-\\!', as.character(nb), sep=''))
}
model <- function(het_model, prime_model, nb_model, het_noise, prime_noise, nb_noise) {
    return(paste(model_('M', het_model, prime_model, nb_model), model_('N', het_noise, prime_noise, nb_noise)))
}
to_tex <- function(name, result) {
    result=paste(name, '$\\sim', result, '$', sep='')
    pad='\\vspace{1cm}'
    result=paste(pad, result, pad)
    print(result)
    return(result)
}

text_size = 3
# Labels definitions
lab = dfsim %>% filter(
    (model == 'A1'     & matrix_size == 300000) |
    (model == 'A2'     & matrix_size == 300000) |
    (model == 'A1het' & matrix_size == 250000) |
    (model == 'C1het' & matrix_size == 250000) |
    (model == 'D1het' & matrix_size == 500000) |
    (model == 'D2het' & matrix_size == 500000)
) 
lab = lab %>% select(gflops, matrix_size, model) %>% distinct()
#lab = lab %>% mutate(label = case_when(model == 'A1'     ~ paste(to_tex('(a) kernels', model(F, F, 1, F, F, 0)), to_tex('network', model(F, T, 1, F, F, 0)), sep='\n'),
#                                       model == 'A2'     ~ paste(to_tex('(b) kernels', model(F, F, 1, F, F, 0)), to_tex('network', model(F, T, 1, F, T, 1)), sep='\n'),
#                                       model == 'A1het' ~ paste(to_tex('(c) kernels', model(F, F, 1, F, F, 0)), to_tex('dgemm', model(T, F, 1, F, F, 0)), to_tex('network', model(F, T, 1, F, F, 0)), sep='\n'),
#                                       model == 'C1het' ~ paste(to_tex('(d) kernels', model(F, F, 1, F, F, 0)), to_tex('dgemm', model(T, F, 2, F, F, 0)), to_tex('network', model(F, T, 1, F, F, 0)), sep='\n'),
#                                       model == 'D1het' ~ paste(to_tex('(e) kernels', model(F, F, 1, F, F, 2)), to_tex('dgemm', model(T, F, 2, T, F, 2)), to_tex('network', model(F, T, 1, F, F, 0)), sep='\n'),
#                                       model == 'D2het' ~ paste(to_tex('(f) kernels', model(F, F, 1, F, F, 2)), to_tex('dgemm', model(T, F, 2, T, F, 2)), to_tex('network', model(F, T, 1, F, T, 1)), sep='\n'),
#                                       T             ~ as.character(model)))
lab = lab %>% mutate(label = case_when(model == 'A1'     ~ paste(to_tex('\\textbf{(a)} kernels', model(F, F, 1, F, F, 0)), to_tex('network', model(F, T, 1, F, F, 0)), sep='\n'),
                                       model == 'A2'     ~ paste('\\textbf{(b)} model (a) with', to_tex('network', model(F, T, 1, F, T, 1)), sep='\n'),
                                       model == 'A1het' ~  paste('\\textbf{(c)} model (a) with', to_tex('dgemm', model(T, F, 1, F, F, 0)), sep='\n'),
                                       model == 'C1het' ~  paste('\\textbf{(d)} model (a) with', to_tex('dgemm', model(T, F, 2, F, F, 0)), sep='\n'),
                                       model == 'D1het' ~  paste('\\textbf{(e)} model (a) with', to_tex('kernels', model(F, F, 1, F, F, 2)), to_tex('dgemm', model(T, F, 2, T, F, 2)), sep='\n'),
                                       model == 'D2het' ~  paste('\\textbf{(f)} model (e) with', to_tex('network', model(F, T, 1, F, T, 1)), sep='\n'),
                                       T             ~ as.character(model)))
lab$x = lab$matrix_size
lab$y = lab$gflops
lab$xl = lab$x*1.0
lab$yl = lab$y*1.0
delta_x = max(dfsim$matrix_size) / 12
delta_y = max(dfsim$gflops) / 12
max_x = max(dfsim$matrix_size)
max_y = max(dfsim$gflops)
lab = lab %>% mutate(xl = case_when(model == 'A1'     ~ 0.25*max_x,
                                    model == 'A2'     ~ 0.8*max_x,
                                    model == 'A1het' ~ 0.3*max_x,
                                    model == 'C1het' ~ 0.68*max_x,
                                    model == 'D1het' ~ 0.38*max_x,
                                    model == 'D2het' ~ 0.8*max_x,
                                       T             ~ xl))
lab = lab %>% mutate(yl = case_when(model == 'A1'     ~ 0.95*max_y,
                                    model == 'A2'     ~ 0.9*max_y,
                                    model == 'A1het' ~ 0.4*max_y,
                                    model == 'C1het' ~ 0.4*max_y,
                                    model == 'D1het' ~ 0.1*max_y,
                                    model == 'D2het' ~ 0.1*max_y,
                                       T             ~ yl))

error_txt = tmp %>% filter(matrix_size == 500000)
error_txt = error_txt %>% filter(model %in% c('A1', 'A1het', 'D2het'))

# Plot drawing
plot = ggplot(dfsim, aes(x=matrix_size, y=gflops)) + geom_point(data=dfreal, shape=1, size=0.5) + geom_point(aes(color=model), size=0.5)
plot = plot + stat_summary(aes(color=model), fun.y='mean', geom='line', linetype='dashed')
plot = plot + stat_summary(fun.y='mean', geom='line', data=dfreal)
plot = plot + theme_bw() + expand_limits(y=0)
plot = plot + scale_color_brewer(type='qual', palette='Dark2', guide=F)
#plot = plot + theme(text=element_text(size=12))
plot = plot + ylab('Gflop/s') + xlab('Matrix rank')
plot = plot + geom_segment(data=lab, aes(x=xl, xend=x, y=yl, yend=y, color=model), linetype="solid")
# the two following lines are a dirty hack, apparently we cannot modify the lightness of an existing color palette
# so I have to use a small alpha, but I did not want to see the background, thus I put white squares behind
plot = plot + geom_label(data=lab, aes(x=xl, y=yl, label=label, group=model), size=text_size, color='white')
plot = plot + geom_label(data=lab, aes(x=xl, y=yl, label=label, group=model, fill=model), alpha=0.3, size=text_size)
plot = plot + scale_fill_brewer(type='qual', palette='Dark2', guide=F)
plot = plot + geom_segment(data=lab, x=max_x*0.75, xend=4.5e5, y=max_y*0.6, yend=mean((dfreal %>% filter(matrix_size == 450000))$gflops), linetype="solid")
plot = plot + geom_label(data=lab, aes(x=max_x*0.75, y=max_y*0.6), label='\\textbf{reality}', size=text_size)
plot = plot + geom_text(data=error_txt, aes(label=pretty_error, y=gflops_simulation+200), x=5.3e5)
plot = plot + expand_limits(x=5.35e5)
plot
tikzTest("A1het")
dev.off()
print(plot)
#ggsave(filename='figures/validation_performance.pdf', plot=plot)
#plot
#+end_src

#+RESULTS:
#+begin_example

[1] "\\vspace{1cm} \\textbf{(a)} kernels$\\sim\\mathcal{M}\\!-\\!1 \\mathcal{N}\\!-\\!0$ \\vspace{1cm}"
[1] "\\vspace{1cm} network$\\sim\\mathcal{M'}\\!-\\!1 \\mathcal{N}\\!-\\!0$ \\vspace{1cm}"
[1] "\\vspace{1cm} network$\\sim\\mathcal{M'}\\!-\\!1 \\mathcal{N'}\\!-\\!1$ \\vspace{1cm}"
[1] "\\vspace{1cm} dgemm$\\sim\\mathcal{M}_{H}\\!-\\!1 \\mathcal{N}\\!-\\!0$ \\vspace{1cm}"
[1] "\\vspace{1cm} dgemm$\\sim\\mathcal{M}_{H}\\!-\\!2 \\mathcal{N}\\!-\\!0$ \\vspace{1cm}"
[1] "\\vspace{1cm} kernels$\\sim\\mathcal{M}\\!-\\!1 \\mathcal{N}\\!-\\!2$ \\vspace{1cm}"
[1] "\\vspace{1cm} dgemm$\\sim\\mathcal{M}_{H}\\!-\\!2 \\mathcal{N}_{H}\\!-\\!2$ \\vspace{1cm}"
[1] "\\vspace{1cm} network$\\sim\\mathcal{M'}\\!-\\!1 \\mathcal{N'}\\!-\\!1$ \\vspace{1cm}"

Measuring dimensions of: \char77
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e815444dc9' '/tmp/Rtmp6r2Il4/tikzDevice60e815444dc9/tikzStringWidthCalc.tex'
Measuring dimensions of: \char103
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e818964e5c' '/tmp/Rtmp6r2Il4/tikzDevice60e818964e5c/tikzStringWidthCalc.tex'
Measuring dimensions of: \char106
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e81fb6f04c' '/tmp/Rtmp6r2Il4/tikzDevice60e81fb6f04c/tikzStringWidthCalc.tex'
Measuring dimensions of: \char112
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e82660c329' '/tmp/Rtmp6r2Il4/tikzDevice60e82660c329/tikzStringWidthCalc.tex'
Measuring dimensions of: \char113
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e84515281d' '/tmp/Rtmp6r2Il4/tikzDevice60e84515281d/tikzStringWidthCalc.tex'
Measuring dimensions of: \char121
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e8a199219' '/tmp/Rtmp6r2Il4/tikzDevice60e8a199219/tikzStringWidthCalc.tex'
Measuring dimensions of: \char81
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e832ffe36f' '/tmp/Rtmp6r2Il4/tikzDevice60e832ffe36f/tikzStringWidthCalc.tex'
Measuring dimensions of: gjpqyQ
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e8785b2678' '/tmp/Rtmp6r2Il4/tikzDevice60e8785b2678/tikzStringWidthCalc.tex'
Measuring dimensions of: \char77
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e81d199d2f' '/tmp/Rtmp6r2Il4/tikzDevice60e81d199d2f/tikzStringWidthCalc.tex'
Measuring dimensions of: \char103
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e8622041ae' '/tmp/Rtmp6r2Il4/tikzDevice60e8622041ae/tikzStringWidthCalc.tex'
Measuring dimensions of: \char106
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e84dcd0373' '/tmp/Rtmp6r2Il4/tikzDevice60e84dcd0373/tikzStringWidthCalc.tex'
Measuring dimensions of: \char112
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e86cce75a1' '/tmp/Rtmp6r2Il4/tikzDevice60e86cce75a1/tikzStringWidthCalc.tex'
Measuring dimensions of: \char113
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e84525c37b' '/tmp/Rtmp6r2Il4/tikzDevice60e84525c37b/tikzStringWidthCalc.tex'
Measuring dimensions of: \char121
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e83dd1526' '/tmp/Rtmp6r2Il4/tikzDevice60e83dd1526/tikzStringWidthCalc.tex'
Measuring dimensions of: \char81
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e8188ee949' '/tmp/Rtmp6r2Il4/tikzDevice60e8188ee949/tikzStringWidthCalc.tex'
Measuring dimensions of: gjpqyQ
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e84205b2df' '/tmp/Rtmp6r2Il4/tikzDevice60e84205b2df/tikzStringWidthCalc.tex'
Measuring dimensions of: Gflop/s
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e84936deb' '/tmp/Rtmp6r2Il4/tikzDevice60e84936deb/tikzStringWidthCalc.tex'
Measuring dimensions of: 0
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e848aa595c' '/tmp/Rtmp6r2Il4/tikzDevice60e848aa595c/tikzStringWidthCalc.tex'
Measuring dimensions of: 10000
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e8579f48dc' '/tmp/Rtmp6r2Il4/tikzDevice60e8579f48dc/tikzStringWidthCalc.tex'
Measuring dimensions of: 20000
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e84cfabe1' '/tmp/Rtmp6r2Il4/tikzDevice60e84cfabe1/tikzStringWidthCalc.tex'
Measuring dimensions of: 30000
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e86acc1d45' '/tmp/Rtmp6r2Il4/tikzDevice60e86acc1d45/tikzStringWidthCalc.tex'
Measuring dimensions of: Matrix rank
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e826fc41ac' '/tmp/Rtmp6r2Il4/tikzDevice60e826fc41ac/tikzStringWidthCalc.tex'
Measuring dimensions of: 1e+05
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e85e6db47a' '/tmp/Rtmp6r2Il4/tikzDevice60e85e6db47a/tikzStringWidthCalc.tex'
Measuring dimensions of: 2e+05
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e8252cef4' '/tmp/Rtmp6r2Il4/tikzDevice60e8252cef4/tikzStringWidthCalc.tex'
Measuring dimensions of: 3e+05
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e899b0c79' '/tmp/Rtmp6r2Il4/tikzDevice60e899b0c79/tikzStringWidthCalc.tex'
Measuring dimensions of: 4e+05
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e85e312ff4' '/tmp/Rtmp6r2Il4/tikzDevice60e85e312ff4/tikzStringWidthCalc.tex'
Measuring dimensions of: 5e+05
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e85d1912d' '/tmp/Rtmp6r2Il4/tikzDevice60e85d1912d/tikzStringWidthCalc.tex'

Active compiler:
	/usr/bin/pdflatex
	pdfTeX 3.14159265-2.6-1.40.19 (TeX Live 2019/dev/Debian)
	kpathsea version 6.3.1/dev

Measuring dimensions of: A1het
Running command: '/usr/bin/pdflatex' -interaction=batchmode -halt-on-error -output-directory '/tmp/Rtmp6r2Il4/tikzDevice60e83c55a5db' '/tmp/Rtmp6r2Il4/tikzDevice60e83c55a5db/tikzStringWidthCalc.tex'
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2019/dev/Debian) (preloaded format=pdflatex)
 restricted \write18 enabled.
entering extended mode
[1] 26.38245

null device 
          1
#+end_example
*** Problematic cooling
Again, the CSV is stored locally. It has been dumped by [[https://github.com/Ezibenroc/calibration_analysis/blob/4e4e6780ca9036dd2aac4fa8bea066848f23ba7c/dahu/smpi_hpl/hpl_simulation_simple.ipynb][this notebook]] (cell 2).

The zip archives for these files are:
- Simulations:
  - Bad  cooling: [[https://github.com/Ezibenroc/calibration_analysis/tree/2f1cc31b9ce7792c94805276d54248c087cfcdbe/dahu/smpi_hpl/paper_sc19/performance/11][these archives]] (a subset of them, in fact).
  - Good cooling: [[https://github.com/Ezibenroc/calibration_analysis/tree/2f1cc31b9ce7792c94805276d54248c087cfcdbe/dahu/smpi_hpl/scaling/1][these archives]].
- Real executions:
  - Bad cooling:
    - [[https://github.com/Ezibenroc/calibration_analysis/tree/2f1cc31b9ce7792c94805276d54248c087cfcdbe/dahu/hpl/grenoble_2019-03-15_1855885.zip][archive 1]],
    - [[https://github.com/Ezibenroc/calibration_analysis/tree/2f1cc31b9ce7792c94805276d54248c087cfcdbe/dahu/hpl/grenoble_2019-03-18_1856059.zip][archive 2]],
    - [[https://github.com/Ezibenroc/calibration_analysis/tree/2f1cc31b9ce7792c94805276d54248c087cfcdbe/dahu/hpl/grenoble_2019-10-30_1891433.zip][archive 3]].
  - Good cooling:
    - [[https://github.com/Ezibenroc/calibration_analysis/tree/2f1cc31b9ce7792c94805276d54248c087cfcdbe/dahu/hpl/grenoble_2018-10-10_1811810.zip][archive 1]],
    - [[https://github.com/Ezibenroc/calibration_analysis/tree/2f1cc31b9ce7792c94805276d54248c087cfcdbe/dahu/hpl/grenoble_2018-10-10_1811847.zip][archive 2]],
    - [[https://github.com/Ezibenroc/calibration_analysis/tree/4e4e6780ca9036dd2aac4fa8bea066848f23ba7c/dahu/hpl/scaling/1][archive directory]].
**** Drawing the plot
#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(dplyr)
options(warn=-1)
df = read.csv('data/prediction/validation/temperature/hpl_matrix_size.csv')
head(df)
#+end_src

#+RESULTS:
: 
:   matrix_size gflops    time       mode cooling       date   jobid
: 1      175000  22940  155.72 Simulation    good 2020-09-07 1946997
: 2      500000  25490 3269.59 Simulation    good 2020-09-07 1946995
: 3       75000  17630   15.96 Simulation    good 2020-09-07 1946996
: 4      300000  24570  732.56 Simulation    good 2020-09-07 1946994
: 5       50000  13960    5.97 Simulation    good 2020-09-07 1946992
: 6      450000  25270 2404.44 Simulation    good 2020-09-07 1947004


Removing the two outliers in the real executions. These two points are
"abnormaly fast", each of them was the first to be made in the sequence of runs
(the machines were not overheating yet).
#+begin_src R :results output :session *R* :exports both
print(length(df$gflops))
df = df %>% filter(matrix_size != 175000 | gflops < 22000 | mode == "Simulation" | cooling == "good")
print(length(df$gflops))
#+end_src

#+RESULTS:
: [1] 468
: 
: [1] 466


We also remove the results from the real run of 2018, the platform appears to
have significantly changed since then.
#+begin_src R :results output :session *R* :exports both
print(length(df$gflops))
df = df %>% filter(date != "2018-10-10")
print(length(df$gflops))
#+end_src

#+RESULTS:
: [1] 466
: 
: [1] 453

Then, we can generate the plot.

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 300 :session *R*
df %>%
    mutate(cooling_txt = ifelse(cooling=="good", "Normal cooling", "Problematic cooling")) %>% {(
    ggplot(.) +
        aes(x=matrix_size, y=gflops, color=mode, fill=mode, shape=mode) +
        geom_point(data=filter(., mode=="Reality"), size=2, alpha=.5, color="black") +
        geom_point(data=filter(., mode!="Reality"), size=2, color="NA") +
        stat_summary(fun.y='mean', geom='line') +
        facet_wrap("cooling_txt") +
        expand_limits(y=0) +
        scale_shape_manual(values=c(23, 21, 21)) +
        ylim(0, 30000) +
        ylab('Gflop/s') +
        xlab('Matrix rank') +
        labs(color="Mode", fill="Mode", shape="Mode") +
        scale_fill_brewer(palette="Dark2", guide = guide_legend()) +
        scale_color_brewer(palette="Dark2", guide = guide_legend()) +
        theme_bw() +
        theme(text=element_text(size=16)) +
        theme(legend.position=c(0.5, 0.07), legend.direction="horizontal", legend.box="horizontal") +
        theme(legend.spacing.x = unit(0.5, 'cm'), legend.spacing.y = unit(0, 'cm'))+
        theme(legend.background = element_rect(color="black", size=0.1)) +
        theme(legend.title = element_blank()) +
        theme(strip.background = element_rect(fill="NA")) +
        guides(color = guide_legend(override.aes=list(linetype=0))) +
        guides(fill = guide_legend(label.position = "bottom"))
    )} -> plot
ggsave(filename='img/prediction/validation/temperature/validation_temperature.pdf', plot=plot)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figurezMfxrY.png]]

*** MPI calibration
**** Downloading and extracting the CSV
See the following entries in Tom's journal for the analysis of those
calibrations:
- Old calibration: [2018-09-11 Tue] and [2018-09-20 Thu] and [2019-03-08 Fri]
  (and more... look for the "remote_calibration.html" attachment)
- New calibration: [2020-08-20 Thu] (MPI RingRong calibration of dahu)

First, we download the archives:
#+begin_src sh :results output :exports both
wget https://github.com/Ezibenroc/calibration_analysis/raw/2f1cc31b9ce7792c94805276d54248c087cfcdbe/dahu/mpi/grenoble_2018-08-29_1808878.zip        -O /tmp/mpi_old_calibration.zip
wget https://github.com/Ezibenroc/calibration_analysis/raw/2f1cc31b9ce7792c94805276d54248c087cfcdbe/dahu/mpi/ring/3/grenoble_2020-08-19_1944943.zip -O /tmp/mpi_new_calibration.zip
du -sh /tmp/mpi*calibration.zip
#+end_src

#+RESULTS:
: 8,4M	/tmp/mpi_new_calibration.zip
: 11M	/tmp/mpi_old_calibration.zip

Then, we extract the relevant CSV files:
#+begin_src sh :results output :exports both
unzip -p /tmp/mpi_old_calibration.zip exp/exp_PingPong.csv > data/prediction/validation/mpi_calibration/mpi_calibration_old.csv
unzip -p /tmp/mpi_new_calibration.zip result.csv > data/prediction/validation/mpi_calibration/mpi_calibration_new.csv
wc -l data/prediction/validation/mpi_calibration/*.csv
du -sh data/prediction/validation/mpi_calibration/*.csv
#+end_src

#+RESULTS:
:   721921 data/prediction/validation/mpi_calibration/mpi_calibration_new.csv
:   724000 data/prediction/validation/mpi_calibration/mpi_calibration_old.csv
:  1445921 total
: 33M	data/prediction/validation/mpi_calibration/mpi_calibration_new.csv
: 26M	data/prediction/validation/mpi_calibration/mpi_calibration_old.csv
**** Drawing the plots
#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(dplyr)
library(patchwork)
library(ggrepel)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
df_old = read.csv("data/prediction/validation/mpi_calibration/mpi_calibration_old.csv", header=F,
    col.names=c("func", "size", "timestamp", "duration")) %>%
    mutate(index=0:(length(func)-1), exp_id=index%/%2) %>%
    group_by(exp_id) %>%
    summarise(size=unique(size), duration=sum(duration)) %>%
    mutate(experiment="Optimistic calibration") %>%
    as.data.frame()

df_old %>% head()
#+end_src

#+RESULTS:
#+begin_example

`summarise()` ungrouping output (override with `.groups` argument)

  exp_id   size   duration             experiment
1      0 154133 5.4884e-05 Optimistic calibration
2      1 154133 6.0543e-05 Optimistic calibration
3      2 154133 5.4087e-05 Optimistic calibration
4      3 154133 5.0987e-05 Optimistic calibration
5      4 154133 5.1168e-05 Optimistic calibration
6      5 154133 5.1763e-05 Optimistic calibration
#+end_example

#+begin_src R :results output :session *R* :exports both
df_new = read.csv("data/prediction/validation/mpi_calibration/mpi_calibration_new.csv") %>%
    rename_at(1,~"func") %>%
    rename_at(2,~"mpi_rank") %>%
    mutate(index=0:(length(func)-1), exp_id=index%/%2, op_kind=exp_id%%2) %>%
    mutate(op_kind=ifelse(op_kind==0, "recv-send", "send-recv")) %>%
    group_by(mpi_rank, op_id, exp_id, op_kind) %>%
    summarise(size=unique(size), duration=sum(duration)) %>%
    mutate(experiment="Realistic calibration")

df_new_remote = df_new %>%
    filter(op_kind=="send-recv", mpi_rank==31) %>% # here we keep only the PingPong 31<->32 started by 31, this is a remote communication
    as.data.frame()

df_new_local = df_new %>%
    filter(op_kind=="send-recv", mpi_rank==12) %>% # here we keep only the PingPong 12<->13 started by 13, this is a local communication
    as.data.frame()

df_new_remote %>% head()
#+end_src

#+RESULTS:
#+begin_example

`summarise()` regrouping output by 'mpi_rank', 'op_id', 'exp_id' (override with `.groups` argument)

  mpi_rank op_id exp_id   op_kind size    duration            experiment
1       31     0 135361 send-recv    8 0.000566898 Realistic calibration
2       31     1 135363 send-recv    8 0.000001457 Realistic calibration
3       31     2 135365 send-recv    8 0.000001190 Realistic calibration
4       31     3 135367 send-recv    8 0.000001124 Realistic calibration
5       31     4 135369 send-recv    8 0.000001272 Realistic calibration
6       31     5 135371 send-recv    8 0.000001118 Realistic calibration
#+end_example

#+begin_src R :results output :session *R* :exports both
df_old %>%
    group_by(size) %>%
    mutate(med_duration = median(duration)) %>%
    ungroup() %>%
    mutate(outlier=duration > 10*med_duration) %>%
    head() %>% as.data.frame()
#+end_src

#+RESULTS:
: 
:   exp_id   size   duration             experiment med_duration outlier
: 1      0 154133 5.4884e-05 Optimistic calibration  5.01175e-05   FALSE
: 2      1 154133 6.0543e-05 Optimistic calibration  5.01175e-05   FALSE
: 3      2 154133 5.4087e-05 Optimistic calibration  5.01175e-05   FALSE
: 4      3 154133 5.0987e-05 Optimistic calibration  5.01175e-05   FALSE
: 5      4 154133 5.1168e-05 Optimistic calibration  5.01175e-05   FALSE
: 6      5 154133 5.1763e-05 Optimistic calibration  5.01175e-05   FALSE

#+begin_src R :results output :session *R* :exports both
draw_plot = function(df, breakpoints, nb_sizes, nb_point_per_size, color) {
    limits = c(0, breakpoints, 1e9)
    set.seed(42)
    # First, let's remove the outliers
    df = df %>%
        group_by(size) %>%
        mutate(med_duration = median(duration)) %>%
        ungroup() %>%
        filter(duration < 10*med_duration)
    sizes = df %>% select(size) %>% unique() %>% sample_n(nb_sizes) %>% pull(size)
    df_aggr = df %>% group_by(size) %>% summarise(duration=mean(duration))
    df_sampled = df %>% filter(size %in% sizes) %>% sample_n(nb_sizes*nb_point_per_size)  # TODO make it so that we have the right amount?
    plot = ggplot() +
        aes(x=size, y=duration) +
        geom_point(data=df_sampled, color=color, alpha=0.1) +
        theme_bw() +
        scale_x_log10() +
        scale_y_log10() +
        expand_limits(x=c(1, 1e9), y=c(1e-6, 1)) +
        xlab("Message size (B)") +
        ylab("Ping-pong duration (s)")
    for(i in 1:(length(breakpoints)+1)) {
        inf = limits[i]
        sup = limits[i+1]
        tmp = df_aggr %>% filter(size > inf, size < sup)
        reg = lm("duration ~ size", data=tmp)
        slope = coef(summary(reg))["size", "Estimate"]
        inter = coef(summary(reg))["(Intercept)", "Estimate"]
        tmp=data.frame(size=c(inf, sup, runif(1000, inf, sup)))
        tmp$duration = inter + tmp$size * slope
        plot = plot +
            geom_line(data=tmp)
    }
    for(b in breakpoints) {
        plot = plot + geom_vline(xintercept=b, linetype="dashed")
    }
    return(plot) #  + theme(plot.margin = unit(c(0,60,0,0), "pt"))
}
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 200 :session *R*
plot = draw_plot(df_old, c(8133, 33956, 63305, 158541), 92, 25, "#d95f02") + ggtitle("Optimistic calibration (remote)")
plot = plot + plot_spacer()
plot = plot + (draw_plot(df_new_remote, c(8000, 5951087, 42672591, 160097505), 92, 25, "#7570b3") +
       ggtitle("Realistic calibration (remote)") +
       theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())
)
plot = plot + (draw_plot(df_new_local, c(16000, 6000000, 36900419, 160097505), 92, 25, "#7570b3") +
       ggtitle("Realistic calibration (local)") +
       theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())
)
ggsave(filename='img/prediction/validation/mpi_calibration/mpi_calibration.png', plot=plot + plot_layout(widths = c(2, .2,2,2)))
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figureTfxZx7.png]]
*** Geometry
For this plot, the CSV files are small, so they are stored directly in the
repository, no need to download them. They have been dumped by [[https://github.com/Ezibenroc/calibration_analysis/blob/98fa2d901f5ea2fd43a140eac6f12c80d37ccb96/dahu/smpi_hpl/simple_geometry.ipynb][this notebook]]
(cell 2).

The zip archives for these files are:
- Real executions: [[https://github.com/Ezibenroc/calibration_analysis/blob/d3aae055b3b296397906e7214eaac4f41445fbb9/dahu/hpl/geometry/4][these archives]].
- Simulations: [[https://github.com/Ezibenroc/calibration_analysis/blob/98fa2d901f5ea2fd43a140eac6f12c80d37ccb96/dahu/smpi_hpl/geometry/33][these archives]].
- Old simulations (wrong network model): [[https://github.com/Ezibenroc/calibration_analysis/blob/d3aae055b3b296397906e7214eaac4f41445fbb9/dahu/smpi_hpl/geometry/13][these archives]].
**** Drawing the plot
#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(dplyr)
library(patchwork)
library(ggrepel)
df = read.csv("data/prediction/validation/geometry/hpl_geometry.csv") %>%
    mutate(network_model="good")
old_df = read.csv("data/prediction/validation/geometry/old_hpl_geometry.csv") %>%
    mutate(network_model="bad") %>%
    filter(mode != "reality") # the real runs are duplicated in the two files
df = rbind(df, old_df) %>%
    mutate(geometry = paste(proc_p, proc_q, sep="×")) %>%
    mutate(network_model = ifelse(network_model=="good", "realistic", "optimistic")) %>%
    mutate(mode = ifelse(mode == "reality", "Reality", paste("Simulation (", network_model, "\nnetwork calibration)", sep="")))
str(df)
#+end_src

#+RESULTS:
#+begin_example

'data.frame':	252 obs. of  9 variables:
 $ filename     : Factor w/ 42 levels "../hpl/geometry/4/grenoble_2020-06-26_1937786.zip",..: 18 10 17 33 26 15 14 31 25 35 ...
 $ matrix_size  : int  250000 250000 250000 250000 250000 250000 250000 250000 250000 250000 ...
 $ proc_p       : int  48 80 30 96 2 160 60 20 8 480 ...
 $ proc_q       : int  20 12 32 10 480 6 16 48 120 2 ...
 $ mode         : chr  "Simulation (realistic\nnetwork calibration)" "Simulation (realistic\nnetwork calibration)" "Simulation (realistic\nnetwork calibration)" "Simulation (realistic\nnetwork calibration)" ...
 $ time         : num  511 626 482 679 896 ...
 $ gflops       : num  20380 16640 21630 15340 11630 ...
 $ network_model: chr  "realistic" "realistic" "realistic" "realistic" ...
 $ geometry     : chr  "48×20" "80×12" "30×32" "96×10" ...
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 300 :session *R*
set.seed(42) # deterministic geom_text_repel
tmp = df %>%
    filter(mode == "Reality") %>%
    group_by(mode, proc_p, proc_q, geometry) %>%
    summarise(gflops = 1/mean(1/gflops)) # harmonic mean since this is a rate

# Uncomment the following
#df = df %>% filter(network_model == "bad" | mode == "Reality")
p = ggplot() +
    aes(x=proc_p, y=gflops, shape=mode, fill=mode) +
    stat_summary(data=df, aes(color=mode), fun.y = mean, geom="line") +
    geom_point(data=df %>% filter(mode=="Reality"), size=2, alpha=.5, color="black") +
    geom_point(data=df %>% filter(mode!="Reality"), size=2, color="NA") +
    geom_text_repel(data=tmp, size=3.5, direction="y", segment.alpha=0.2, nudge_y=-1500, aes(label=geometry, y=gflops)) +
    geom_label_repel(data=df %>% filter(proc_p == 2 & network_model == "optimistic"), label="Optimistic calibration", nudge_y=+5000, nudge_x=-.27, segment.alpha=0.5, show_guide  = F, alpha=.7) +
    geom_label_repel(data=df %>% filter(proc_p == 2 & network_model == "optimistic"), label="Optimistic calibration", nudge_y=+5000, nudge_x=-.27, segment.alpha=0, show_guide  = F, fill=NA) +    
#    geom_label_repel(data=df %>% filter(proc_p == 3 & network_model == "realistic" & mode!= "Reality"), aes(label="Realistic\ncalibration"), nudge_y=+3500, nudge_x=-500, segment.alpha=0.5, show_guide  = F) +
    geom_label_repel(data=df %>% filter(proc_p == 3 & network_model == "realistic" & mode!= "Reality"), label="Realistic\ncalibration", nudge_y=+300, nudge_x=-.27, segment.alpha=0.5, show_guide  = F, alpha=.7,seed=1234) +
    geom_label_repel(data=df %>% filter(proc_p == 3 & network_model == "realistic" & mode!= "Reality"), label="Realistic\ncalibration", nudge_y=+300, nudge_x=-.27, segment.alpha=0, show_guide  = F, fill=NA,seed=1234) +
    theme_bw() +
    ylab("Gflop/s") +
    xlab("P") +
    labs(color="Mode", fill="Mode", shape="Mode") +
    scale_fill_brewer(palette="Dark2", guide = guide_legend()) +
    scale_color_brewer(palette="Dark2", guide = guide_legend()) +
    scale_shape_manual(values=c(23, 21, 21)) +
    scale_x_log10() +
    expand_limits(y=0) +
    theme(legend.position=c(0.4, 0.1), legend.direction="horizontal", legend.box="horizontal") +
    theme(legend.spacing.x = unit(0.5, 'cm'), legend.spacing.y = unit(0, 'cm'))+
    theme(legend.background = element_rect(color="black", size=0.1)) +
    theme(legend.title = element_blank()) +
    guides(color = guide_legend(override.aes=list(linetype=0))) +
    guides(fill = guide_legend(label.position = "bottom"))

ggsave("img/prediction/validation/geometry/validation_geometry.pdf", plot=p, device=cairo_pdf)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figureL8vch7.png]]
*** Factorial experiment
Again, the CSV files are small, so they are stored directly in the
repository. They have been dumped by [[https://github.com/Ezibenroc/calibration_analysis/blob/eb857051cb94e2f4390cedb002ed491a43d3c914/dahu/smpi_hpl/hpl_factors.ipynb][this notebook]] (cell 2).

The zip archives for these files are:
- Simulations: [[https://github.com/Ezibenroc/calibration_analysis/tree/eb857051cb94e2f4390cedb002ed491a43d3c914/dahu/smpi_hpl/factor_exp/2][these archives]]
- Real executions: [[https://github.com/Ezibenroc/calibration_analysis/tree/eb857051cb94e2f4390cedb002ed491a43d3c914/dahu/hpl/factor_exp/2][these archives]]
**** Drawing the plot
#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(dplyr)
library(patchwork)
df = read.csv("data/prediction/validation/factorial/hpl_factorial.csv")
str(df)
#+end_src

#+RESULTS:
#+begin_example

'data.frame':	648 obs. of  11 variables:
 $ filename   : Factor w/ 24 levels "../hpl/factor_exp/2/grenoble_2020-01-20_1912032.zip",..: 23 22 14 14 14 14 14 14 14 14 ...
 $ matrix_size: int  250000 250000 250000 250000 250000 250000 250000 250000 250000 250000 ...
 $ gflops     : num  21160 25240 20890 21850 21770 ...
 $ time       : num  492 413 499 477 478 ...
 $ mode       : Factor w/ 2 levels "Reality","Simulation": 2 2 2 2 2 2 2 2 2 2 ...
 $ bcast      : int  0 1 0 2 5 5 0 2 3 4 ...
 $ block_size : int  128 128 256 256 256 256 256 256 256 256 ...
 $ depth      : int  1 0 0 0 0 1 0 1 0 1 ...
 $ swap       : int  1 1 0 2 1 0 2 2 0 0 ...
 $ pfact      : int  1 1 1 1 1 1 1 1 1 1 ...
 $ rfact      : int  2 2 2 2 2 2 2 2 2 2 ...
#+end_example

#+begin_src R :results output :session *R* :exports both
df %>%
    filter(matrix_size == 250000) %>%
    filter(block_size == 128) %>%
    filter(depth == 1) %>%
    filter(bcast == 2) %>%
    filter(swap == 0)
#+end_src

#+RESULTS:
#+begin_example

                                             filename matrix_size gflops   time
1        factor_exp/2/grenoble_2020-01-16_1909235.zip      250000  24880 418.73
2 ../hpl/factor_exp/2/grenoble_2020-01-22_1912418.zip      250000  24010 433.92
3 ../hpl/factor_exp/2/grenoble_2020-01-26_1912919.zip      250000  23880 436.21
4 ../hpl/factor_exp/2/grenoble_2020-01-20_1912032.zip      250000  23970 434.55
5 ../hpl/factor_exp/2/grenoble_2020-01-24_1912915.zip      250000  23950 434.86
6 ../hpl/factor_exp/2/grenoble_2020-01-25_1912917.zip      250000  23890 436.03
7 ../hpl/factor_exp/2/grenoble_2020-01-25_1912916.zip      250000  23920 435.53
8 ../hpl/factor_exp/2/grenoble_2020-01-26_1912918.zip      250000  23870 436.48
9 ../hpl/factor_exp/2/grenoble_2020-01-26_1912920.zip      250000  24070 432.76
        mode bcast block_size depth swap pfact rfact
1 Simulation     2        128     1    0     1     2
2    Reality     2        128     1    0     1     2
3    Reality     2        128     1    0     1     2
4    Reality     2        128     1    0     1     2
5    Reality     2        128     1    0     1     2
6    Reality     2        128     1    0     1     2
7    Reality     2        128     1    0     1     2
8    Reality     2        128     1    0     1     2
9    Reality     2        128     1    0     1     2
#+end_example

The factors are, from most significant to least:
- block_size
- depth
- bcast
- broadcast
- swap

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 300 :session *R*
# #+begin_src R :results output :session *R* :exports both
df = df %>% mutate(Depth = depth) # using capital letters everywhere
dfreal = df %>%
    filter(mode == "Reality") %>%
    group_by(bcast, block_size, swap, Depth) %>%
    summarise(gflops = 1/mean(1/gflops)) # harmonic mean since this is a rate
tmp = df %>%
    filter(mode == "Simulation") %>%
    full_join(dfreal, by=c("bcast", "block_size", "swap", "Depth"), suffix=c("_simulation", "_reality")) %>%
    mutate(error = abs(gflops_simulation - gflops_reality) / gflops_reality * 100) %>%
    mutate(error_str = ifelse(
        error > 10, "> 10%", ifelse(
        error > 5, "5% - 10%",
        "0% - 5%")))
tmp$error_str = factor(tmp$error_str,
        levels = c("0% - 5%", "5% - 10%", "> 10%"))

do_plot = function(df, tmp, bsize, label_y) {
    swap_func = function(x) {
        x = as.integer(substring(x, 1, 1)) # x is interaction(swap, bcast), so this just returns swap
        result = ifelse(x==0, "bin-exch",
                 ifelse(x==1, "long",
                 ifelse(x==2, "mix",
                 "ERROR"
        )))
    }
    original_tmp = tmp
    ymin = 0 # min(df$gflops)
    ymax = max(df$gflops)
    stopifnot(label_y >= ymin)
    stopifnot(label_y <= ymax)
    xmin_rect = c(1,3,5)*3+0.5
    df = df %>% filter(block_size == bsize)
    tmp = tmp %>% filter(block_size == bsize)
    return(ggplot() +
    aes(x=interaction(swap, bcast, sep=":"), y=gflops, shape=mode, fill=mode) +
    geom_rect(
        data=data.frame(xmin=xmin_rect, xmax=xmin_rect+3),
        aes(xmin=xmin, xmax=xmax, x=NULL, y=NULL, shape=NULL),
        fill="black", alpha=0.2, ymin=-as.double("inf"), ymax=as.double("inf")
    ) +
    # The following is an invisible geom_segment to get the legend right if there misses some values in the given data.
    geom_segment(data=original_tmp, aes(xend=interaction(swap, bcast, sep=":"), y=gflops_reality, yend=gflops_simulation, color=error_str), alpha=0) +
    geom_segment(data=tmp, size=1.5, aes(xend=interaction(swap, bcast, sep=":"), y=gflops_reality, yend=gflops_simulation, color=error_str)) +
    geom_point(data=df %>% filter(mode=="Reality"), size=2, alpha=.5, color="black") +
    geom_point(data=df %>% filter(mode=="Simulation"), size=2, color="NA") +
    geom_label(
        data=data.frame(txt=c("1rg", "1rM", "2rg", "2rM", "Lng", "LnM"),
                          x=c("1:0", "1:1", "1:2", "1:3", "1:4", "1:5")),
        aes(label=txt, x=x, shape=NULL),
        size=2, y=label_y, fill="white"#, family="mono", 
    ) +
    theme_bw() +
    facet_wrap("Depth", labeller="label_both", nrow=1) +
    ylab("Gflop/s") +
    xlab("Swap") +
    labs(color="Error", fill="Mode", shape="Mode") +
    theme(axis.text.x = element_text(angle=60, hjust=1, size=6)) + #, family="mono"
    theme(strip.background = element_rect(fill="NA")) +
#    theme(legend.position = "bottom") +
    scale_fill_brewer(palette="Dark2", guide = guide_legend()) +
    scale_shape_manual(values=c(23, 21)) +
    scale_color_brewer(palette="BuPu", guide = guide_legend()) +
    scale_x_discrete(labels = swap_func) +
    theme(legend.position="bottom", legend.direction="horizontal") +
    theme(legend.spacing.x = unit(0.5, 'cm'), legend.spacing.y = unit(0, 'cm'))+
    theme(legend.background = element_rect(color="black", size=0.1)) +
    guides(color = guide_legend(label.position = "bottom")) +
    guides(fill = guide_legend(label.position = "bottom")) +
    expand_limits(y=c(ymin, ymax)) +
    ggtitle(paste("Block size:", bsize)) +
    theme(plot.title = element_text(hjust = 0.5))
    )
}

p128 = do_plot(df, tmp, 128, label_y=18500) +
    geom_rect(data=data.frame(Depth=1), aes(x=NULL, y=NULL, shape=NULL), xmin=6, xmax=8, ymin=23000, ymax=25700, linetype="dashed", alpha=0, fill="black", color="black")
p256 = do_plot(df, tmp, 256, label_y=24500) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())

layout <- c(
    area(t = 0, l = 0, b = 20, r = 30),
    area(t = 0, l = 31, b = 20, r = 61),
    area(t=15, l=10, b=15, r=50)
)
# For visualizing the layout, uncomment the next line and comment the following one
#p=plot(layout)
p = p128 + p256 + guide_area() + plot_layout(design = layout, guides="collect")
ggsave("img/prediction/validation/factorial/validation_factorial.pdf", plot=p, device=cairo_pdf)
p
#+end_src

#+RESULTS:
[[file:/tmp/babel-1gNT9F/figurefutQBh.png]]
* Experimental control
** Parameter space
No dataset here, we generate the data.
#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(ggplot2)
library(patchwork)
set.seed(42)

N = 100000
max_prod = 1e10
max_size = 1e4

# First, we generate a dataframe with 'independent' sizes (not truly independent, we reject the
# rows where the product is larger than the limit.
df_inde = data.frame()
while(nrow(df_inde) < N) {
    rem = N-nrow(df_inde)
    tmp = data.frame(m=round(runif(rem, 1, max_size)), n=round(runif(rem, 1, max_size)), k=round(runif(rem, 1, max_size))) %>%
        mutate(prod=m*n*k) %>%
        filter(prod <= max_prod)
    df_inde = rbind(df_inde, tmp)
}
df_inde$mode = "Independent sizes"

# Then, we generate a dataframe with uniform product, again using rejection sampling.
N_unif = N %/% 6 # 6 permutations
df_unif = data.frame()
while(nrow(df_unif) < N_unif) {
    rem = N_unif - nrow(df_unif)
    tmp = data.frame(prod=runif(rem, 1, max_prod)) %>%
        mutate(A=runif(rem, 1, prod**(1/3))) %>%
        mutate(B=runif(rem, 1, (prod/A)**(1/2))) %>%
        mutate(C=prod/A/B) %>%
        filter(A <= max_size, B <= max_size, C <= max_size) %>%
        mutate(A=round(A), B=round(B), C=round(C), prod=A*B*C)
    df_unif = rbind(df_unif, tmp)
}
df_unif = rbind(
    df_unif %>% mutate(m=A, n=B, k=C),
    df_unif %>% mutate(m=A, n=C, k=B),
    df_unif %>% mutate(m=B, n=A, k=C),
    df_unif %>% mutate(m=C, n=A, k=B),
    df_unif %>% mutate(m=B, n=C, k=A),
    df_unif %>% mutate(m=C, n=B, k=A)) %>%
    select(-A, -B, -C) %>%
    mutate(mode="Uniform product")

# Finally we merge the two dataframes
df = rbind(df_unif, df_inde)
str(df)
#+end_src

#+RESULTS:
: 
: 'data.frame':	199996 obs. of  5 variables:
:  $ prod: num  3.28e+09 7.12e+09 6.57e+09 6.81e+07 7.44e+08 ...
:  $ m   : num  1338 1885 228 296 14 ...
:  $ n   : num  1031 843 4520 441 6449 ...
:  $ k   : num  2378 4480 6374 522 8245 ...
:  $ mode: chr  "Uniform product" "Uniform product" "Uniform product" "Uniform product" ...

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 700 :height 300 :session *R*
base_plot = df %>%
    ggplot() +
    geom_histogram(bins=50, position="identity", alpha=0.5, boundary=0) +
    theme_bw() +
    ylab('Count') +
    scale_fill_brewer(palette="Dark2")
p1 = base_plot +
    aes(x=prod, fill=mode) +
    xlab('MNK') +
    theme(legend.background = element_rect(color="black", size=0.1)) +
    theme(legend.position=c(0.7, 0.82), legend.title=element_blank())
p2 = base_plot +
    aes(x=m, fill=mode) +
    xlab('M') +
    scale_y_continuous(position = "right") +
    theme(legend.position="none")
plot = p1 + p2
ggsave(filename='img/experiment/parameter_space/distribution.pdf', plot=plot, width=7, height=3)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-iDrifC/figureEsywBF.png]]

** Randomizing the order
The CSV file used in this section has been dumped by this [[https://github.com/Ezibenroc/calibration_analysis/blob/afc789cbbcc3fdd2cb3c02a8837e2a7fbb0604b2/grvingt/grvingt_proper_randomization.ipynb][notebook]] (cell 2)
using the archives: [[https://github.com/Ezibenroc/calibration_analysis/blob/166ba54222891073608059c3e576b30e7cd0b3ff/grvingt/nancy_2018-07-24_1621460.zip][no randomization]], [[https://github.com/Ezibenroc/calibration_analysis/blob/166ba54222891073608059c3e576b30e7cd0b3ff/grvingt/nancy_2018-07-27_1625117.zip][half randomization]], [[https://github.com/Ezibenroc/calibration_analysis/blob/166ba54222891073608059c3e576b30e7cd0b3ff/grvingt/nancy_2018-08-03_1645238.zip][full randomization]].
*** Plot drawing
#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(ggplot2)
df = read.csv('data/experiment/randomizing_order/mpi_calibration_order.csv')
str(df)
#+end_src

#+RESULTS:
: 
: 'data.frame':	1041500 obs. of  5 variables:
:  $ index   : int  0 1 2 3 4 5 6 7 8 9 ...
:  $ start   : num  0.00316 0.00408 0.00499 0.0059 0.00682 ...
:  $ msg_size: int  765921 765921 765921 765921 765921 765921 765921 765921 765921 765921 ...
:  $ duration: num  7.70e-05 7.32e-05 7.29e-05 7.33e-05 7.28e-05 ...
:  $ shuffled: Factor w/ 3 levels "full","half",..: 3 3 3 3 3 3 3 3 3 3 ...

#+begin_src R :results output :session *R* :exports both
df %>%
    group_by(shuffled) %>%
    summarise(max_size=max(msg_size)) %>%
    as.data.frame() -> tmp
max_size = tmp %>% pull(max_size) %>% min()
str(max_size)
tmp
#+end_src

#+RESULTS:
: 
: `summarise()` ungrouping output (override with `.groups` argument)
: 
:  int 989921
: 
:   shuffled max_size
: 1     full  9981108
: 2     half   989921
: 3     none   989921

We used a larger parameter space in one of the runs. For the plots, we will
restrict every mode to the same size interval.

In fact, we will only plot the half-shuffled and the full-shuffled.

#+begin_src R :results output :session *R* :exports both
print(length(df$msg_size))
df = df %>%
    filter(msg_size <= max_size) %>%
    filter(shuffled %in% c("half", "none")) %>%
    mutate(mode=ifelse(shuffled=="none", "Not shuffled", "Shuffled"))
print(length(df$msg_size))
#+end_src

#+RESULTS:
: [1] 1041500
: 
: [1] 662000

#+begin_src R :results output :session *R* :exports both
plot_mpi = function(df, alpha) {
    return(df %>%
        ggplot() +
        aes(x=msg_size, y=duration) +
        geom_point(alpha=alpha) +
        scale_x_log10() +
        scale_y_log10() +
        theme_bw() +
        labs(x="Message size (bytes)", y="Duration (seconds)") +
        facet_wrap("mode")
    )
}
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = df %>% plot_mpi(alpha=0.1)
ggsave(filename='img/experiment/randomizing_order/raw_data.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-70i53c/figurelb3LJj.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = df %>%
    group_by(msg_size, mode) %>%
    summarize(duration=mean(duration)) %>%
    plot_mpi(alpha=1) + expand_limits(y=c(min(df$duration), max(df$duration)))
ggsave(filename='img/experiment/randomizing_order/aggregated_data.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-70i53c/figure1nJZNB.png]]

Let's take again this aggregated data and build two lists, one of sizes that are
in the lower mode, one of sizes that are in the upper mode.
#+begin_src R :results output :session *R* :exports both
zoom = df %>%
    filter(duration <= 3.5e-6) %>% # removing the 'outliers'
    filter(msg_size >= 100, msg_size <= 1000) %>%
    group_by(msg_size, mode) %>%
    mutate(mean_duration=mean(duration)) %>%
    ungroup()
p = 0.2
bounds = zoom %>%
    filter(mode=="Not shuffled") %>%
    pull(mean_duration) %>%
    quantile(probs=c(p, 1-p))
zoom = zoom %>%
    mutate(category = ifelse(mean_duration <= bounds[[1]], "low", ifelse(mean_duration >= bounds[[2]], "high", "medium")))
selection_size = 3
low_selection = zoom %>%
    filter(category=="low") %>%
    pull(msg_size) %>%
    unique() %>%
    sort() 
high_selection = zoom %>%
    filter(category=="high") %>%
    pull(msg_size) %>%
    unique() %>%
    sort()
low_selection_sample = low_selection  %>% .[. <= 800] %>% tail(n=selection_size)
high_selection_sample = high_selection  %>% .[. <= 800] %>% tail(n=selection_size)
selection = c(low_selection_sample, high_selection_sample)
str(selection)
selection_df = zoom %>%
    mutate(category = ifelse(msg_size %in% low_selection, paste(low_selection_sample, collapse=","), paste(high_selection_sample, collapse=","))) %>%
    mutate(category = paste("{", category, "}", sep="")) %>%
    filter(msg_size %in% selection)
#+end_src

#+RESULTS:
: 
:  int [1:6] 703 767 779 705 741 782

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
library(see) # https://github.com/easystats/see
library(ggbeeswarm)
library(cowplot)
plot = selection_df %>%
    ggplot() +
    aes(x=factor(msg_size), y=duration, color=category) +
    geom_quasirandom(alpha=0.5, dodge.width=1, key_glyph=rectangle_key_glyph(fill=color)) +
    stat_summary(fun="mean", geom="point", color="black", size=4, shape=4) +
    facet_wrap("mode") +
    scale_color_brewer(palette="Dark2") +
    theme_bw() +
    expand_limits(y=0) +
    theme(legend.position="bottom") +
    theme(legend.background = element_rect(color="black", size=0.3)) +
    labs(x="Message size (bytes)", y="Duration (seconds)", color="Message size (bytes)") +
    guides(color=guide_legend(override.aes=list(alpha=1)))
ggsave(filename='img/experiment/randomizing_order/distribution.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-70i53c/figureCRwfze.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = selection_df %>%
    ggplot() +
    aes(x=start, y=duration, color=category) +
    geom_point(alpha=0.5, key_glyph=rectangle_key_glyph(fill=color)) +
    facet_wrap(c("mode", "category"), ncol=4) +
    scale_color_brewer(palette="Dark2") +
    theme_bw() +
    expand_limits(y=0) +
    labs(x="Timestamp (seconds)", y="Duration (seconds)", color="Message size (bytes)") +
    theme(legend.position="bottom") +
    theme(legend.background = element_rect(color="black", size=0.3)) +
    guides(color=guide_legend(override.aes=list(alpha=1)))
ggsave(filename='img/experiment/randomizing_order/evolution.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-70i53c/figureJMCmc9.png]]

Now let's have a quick look at the sizes *before* and *after* the low-duration
calls.
#+begin_src R :results output :session *R* :exports both
df %>%
    filter(mode == 'Not shuffled') %>%
    filter(index %% 10 == 0) %>%
    arrange(index) %>%
    mutate(size_before=lag(msg_size, n=1), size_after=lead(msg_size, n=1)) %>%
    filter(msg_size %in% low_selection) %>%
    group_by(msg_size) %>%
    summarise(duration=mean(duration), size_before=unique(size_before), size_after=unique(size_after)) %>%
    as.data.frame()
#+end_src

#+RESULTS:
#+begin_example

`summarise()` regrouping output by 'msg_size' (override with `.groups` argument)
   msg_size    duration size_before size_after
1       102 1.08950e-06          50        591
2       195 1.29450e-06      667582     254743
3       211 1.16998e-06       16620       9852
4       232 1.21954e-06      495560          3
5       260 1.19262e-06      626942         31
6       279 1.14658e-06        1275          6
7       310 1.18878e-06       12657       6825
8       357 1.20886e-06          30       4640
9       366 1.16614e-06          48       5878
10      383 1.15176e-06          26         29
11      400 1.21260e-06           1       7921
12      411 1.16458e-06         703          3
13      444 1.15260e-06          41          7
14      451 1.18381e-06           3       2930
15      451 1.18381e-06          31     379763
16      476 1.13258e-06          29       8504
17      568 1.14188e-06         779         41
18      601 1.19500e-06        2443     500497
19      646 1.27478e-06      697005      24379
20      679 1.13138e-06        2422         50
21      703 1.12164e-06        1335        411
22      767 1.13044e-06        1054     283616
23      779 1.22874e-06      699352        568
24      960 1.15592e-06        1943       1273
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 700 :height 300 :session *R*
start_exp=14.
duration_exp=0.2
seg_length=2e-7
seg_height=3.8e-6
seg_width=0.1
plot = df %>%
    filter(mode == "Not shuffled", start >= start_exp, start <= start_exp+duration_exp, duration <= 4e-6) %>%
    filter(msg_size <= 1000) %>%
    mutate(category_short = ifelse(duration >= 1.7e-6, "Slow", "Fast")) %>%
    mutate(category = ifelse(category_short=="Slow", "More than 1.7µs", "Less than 1.7µs")) %>% {(
    ggplot(.) +
    aes(x=start, xintercept=start, y=duration, color=category) +
 #   geom_rug(sides="t", position="jitter", alpha=0.5, length=unit(0.1, "npc")) +  # <- very nice, but I needed to not overlap the two colors, so using geom_segment instead
    geom_segment(data=filter(., category_short=="Slow"), size=seg_width, mapping=aes(xend=start, y=seg_height, yend=seg_height+seg_length)) +
    geom_segment(data=filter(., category_short=="Fast"), size=seg_width, mapping=aes(xend=start, y=seg_height+seg_length/2, yend=seg_height-seg_length/2)) +
    geom_point(alpha=0.5, key_glyph=rectangle_key_glyph(fill=color)) +
    scale_color_brewer(palette="Dark2") +
    theme_bw() +
    labs(x="Timestamp (seconds)", y="Duration (seconds)") +
    theme(legend.position="bottom", legend.title=element_blank()) +
    theme(legend.background = element_rect(color="black", size=0.3)) +
    coord_cartesian(ylim = c(9e-7*0, 3.5e-6), clip="off") +
    theme(plot.margin = unit(c(15+5.5,5.5,5.5,5.5), "pt")) +
    guides(color=guide_legend(override.aes=list(alpha=1)))
)}
ggsave(filename='img/experiment/randomizing_order/evolution_rug.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-70i53c/figurewYqaRU.png]]
** Randomizing the sizes
*** Generation method
The CSV files used in this section have been dumpted by this [[https://github.com/Ezibenroc/calibration_analysis/blob/19da3d11a4f559a5742b2e9aba689fdad2909708/dahu/blas/expfile_influence.ipynb][notebook]] (cells 8,
9, 10 and 21) using these [[https://github.com/Ezibenroc/calibration_analysis/tree/19da3d11a4f559a5742b2e9aba689fdad2909708/dahu/blas/expfile_influence/2][archives]] Relevant entry in my journal: 2020-12-01.
**** Temporal evolution of the average performance and the DRAM power consumption
#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(ggplot2)
library(anytime)
library(patchwork)
library(scales)
library(ggrepel)
library(ggbeeswarm)

do_plot = function(frame, y_val, y_label) {
    frame = frame %>%
        mutate(exp_kind=stringr::str_to_title(exp_kind))
    avg = frame %>%
        group_by(exp_kind) %>%
        summarise(val=mean(.data[[y_val]]))
    plot = frame %>%
        ggplot() +
        aes_string(x="exp_kind", y=y_val, color="exp_kind", fill="exp_kind") +
        ylab(y_label) +
        theme_bw() +
        expand_limits(y=c(min(frame[[y_val]]), max(frame[[y_val]]))) +
        scale_fill_brewer(type='qual', palette='Dark2') +
        scale_color_brewer(type='qual', palette='Dark2') +
        theme(legend.position="none") +
        guides(fill = guide_legend(override.aes = list(alpha=1, shape=21, size=4))) +
        geom_boxplot(outlier.alpha=0, alpha=0.3) +
        geom_quasirandom(dodge.width=1) +
        xlab('Generation method') +
        ylab(y_label) +
        coord_flip()
    return(plot)
}
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
set.seed(27)
p1 = read.csv("data/experiment/randomizing_sizes/method/dgemm_agr_data.csv") %>%
    filter(node==5, cpu==1) %>%
    do_plot("avg_gflops", "Average performance (Gflop/s)")
p2 = read.csv("data/experiment/randomizing_sizes/method/dgemm_agr_monitoring.csv") %>%
    filter(node==5, cpu==1, kind=="power", subgroup=="dram") %>%
    do_plot("value", "Average DRAM power consumption (W)") +
    theme(axis.text.y = element_blank(), axis.title.y = element_blank())
plot = p1 + p2
ggsave(filename='img/experiment/randomizing_sizes/method/average.pdf', plot=plot, width=7, height=2)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figureMw9Pn8.png]]

*** Expfile
The CSV files used in this section have been dumpted by this [[https://github.com/Ezibenroc/calibration_analysis/blob/ab23ec7e2624a5ca524838a7a9f613252aec0797/dahu/blas/expfile_influence.ipynb][notebook]] (cells 8,
9, 10 and 21) using these [[https://github.com/Ezibenroc/calibration_analysis/tree/dbd0d284a878d4bf4a7e4d86bd6fe590c5f35585/dahu/blas/expfile_influence/1][archives]]. Relevant entry in my journal: 2020-11-25.
**** Temporal evolution of the average performance and the DRAM power consumption
#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(ggplot2)
library(anytime)
library(patchwork)
library(scales)
library(ggrepel)
set.seed(42)

do_plot = function(frame, y_val, y_label) {
    avg = frame %>%
        group_by(Expfile) %>%
        summarise(avg=mean(.data[[y_val]])) %>%
        mutate(start_time=frame %>% pull(start_time) %>% min())
    basic_plot = frame %>%
        ggplot() +
        aes_string(y=y_val, color="Expfile", fill="Expfile") +
        ylab(y_label) +
        theme_bw() +
        expand_limits(y=c(min(frame[[y_val]]), max(frame[[y_val]]))) +
        scale_fill_brewer(type='qual', palette='Dark2') +
        scale_color_brewer(type='qual', palette='Dark2') +
        theme(legend.position="none") +
        guides(fill = guide_legend(override.aes = list(alpha=1, shape=21, size=4)))

    p1 = basic_plot +
        geom_boxplot(aes(x=Expfile), outlier.alpha=0, alpha=0.3) +
        theme(legend.position='none') +
        theme(plot.margin = unit(c(0,0,0,0), "pt"), axis.ticks.length=unit(0, "null")) +
        xlab('Experiment file') +
        ylab(y_label)
    p2 = basic_plot +
        geom_point(shape=21, aes(x=start_time), size=5, alpha=0.7, stroke=0, color="black") +
        scale_x_datetime(breaks = date_breaks("1 day")) +
        theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.y=element_blank()) +
        theme(plot.margin = unit(c(0,0,0,0), "pt"), axis.ticks.length=unit(0, "null")) +
        xlab('Timestamp')
    p_label = basic_plot +
        geom_label(data=avg, aes(x=0, y=avg, fill=NA, color=Expfile, label=Expfile)) +
        theme(plot.margin = unit(c(0,0,0,0), "pt"), axis.ticks.length=unit(0, "null")) +
        theme_void() +
        theme(legend.position="none")

    plot = p1 + p_label + p2 + plot_layout(widths=c(5,1,20), guides = 'collect')
    return(plot)
}
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = read.csv("data/experiment/randomizing_sizes/expfile/dgemm_agr_data.csv") %>%
    filter(node==5, cpu==1) %>%
    mutate(start_time=anytime(start_time)) %>%
    mutate(Expfile=ifelse(expfile=="exp_dgemm_a.csv", "A", ifelse(expfile=="exp_dgemm_c.csv", "B", "C"))) %>%
    do_plot("avg_gflops", "Average performance (Gflop/s)")
ggsave(filename='img/experiment/randomizing_sizes/expfile/average_performance.pdf', plot=plot, width=7, height=3)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figureK6N2u4.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = read.csv("data/experiment/randomizing_sizes/expfile/dgemm_agr_monitoring.csv") %>%
    filter(node==5, cpu==1, kind=="power", subgroup=="dram") %>%
    mutate(start_time=anytime(start_time)) %>%
    mutate(Expfile=ifelse(expfile=="exp_dgemm_a.csv", "A", ifelse(expfile=="exp_dgemm_c.csv", "B", "C"))) %>%
    do_plot("value", "Average DRAM power consumption (W)")
ggsave(filename='img/experiment/randomizing_sizes/expfile/average_power.pdf', plot=plot, width=7, height=3)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figurerKxE1I.png]]
**** Distribution of the regression coefficients
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
library(dplyr)
library(ggplot2)
library(anytime)
library(patchwork)
library(scales)
library(ggrepel)

plot_bivariate <- function(df, x_col, y_col, color_col) {
    density_plot = ggplot() +
        aes_string(color=color_col, group=color_col) +
        geom_density(data=df) +
        theme_void()

    plot_top = density_plot +
        aes_string(x=x_col)
    plot_right = density_plot +
        aes_string(x=y_col) +
        coord_flip()

    text = df %>%
        group_by_(color_col) %>%
        summarise_all(funs(mean))

    scatter_plot = ggplot(df) +
        aes_string(x=x_col, y=y_col, color=color_col, group=color_col) +
        geom_point() +
        stat_ellipse() +
# Uncomment the following line to put labels on the ellipses
        geom_label(data=text, aes_string(label=color_col), alpha=0.7) +
        theme_minimal() +
        scale_x_continuous(breaks = scales::pretty_breaks(n = 3)) +
        scale_y_continuous(breaks = scales::pretty_breaks(n = 3)) +
        xlab(toupper(x_col)) +
        ylab(toupper(y_col))

    return(
        plot_top + plot_spacer() + scatter_plot + plot_right +
        plot_layout(widths = c(4, 1), heights = c(1, 4)) &
        scale_color_brewer(palette="Dark2", type="qual") &
        theme(legend.position='none')
    )
}

plot = read.csv("data/experiment/randomizing_sizes/expfile/dgemm_agr_data.csv") %>%
    filter(node==5, cpu==1) %>%
    mutate(Expfile=ifelse(expfile=="exp_dgemm_a.csv", "A", ifelse(expfile=="exp_dgemm_c.csv", "B", "C"))) %>%
    mutate(cpu_id=interaction(node, cpu), group=interaction(cpu_id, expfile)) %>%
    plot_bivariate("mnk", "nk", "Expfile")
ggsave(filename='img/experiment/randomizing_sizes/expfile/average_distribution.pdf', plot=plot, width=7, height=5)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-b2kpD9/figurehDHrWR.png]]
**** Individual durations
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
library(dplyr)
library(ggplot2)

plot = read.csv("data/experiment/randomizing_sizes/expfile/dgemm_raw_data.csv") %>%
    filter(node==5, cpu==1) %>%
    mutate(MNK=as.numeric(m)*as.numeric(n)*as.numeric(k), gflops=2*MNK*1e-9/duration) %>%
    mutate(Expfile=ifelse(expfile=="exp_dgemm_a.csv", "A", ifelse(expfile=="exp_dgemm_c.csv", "B", "C"))) %>%
    ggplot() +
    aes(x=MNK, y=duration, color=Expfile) +
    geom_point(alpha=0.1) +
    facet_wrap("Expfile") +
    scale_color_brewer(palette="Dark2") +
    geom_abline(slope=6.7e-11) +
    theme_bw() +
    ylab("Duration (seconds)") +
    scale_x_continuous(breaks = c(0e9, 4e9, 8e9)) +
    theme(legend.position="none")
ggsave(filename='img/experiment/randomizing_sizes/expfile/raw_data.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-b2kpD9/figurev1kz6K.png]]

*** Fixing K
The CSV files used in this section have been dumpted by this [[https://github.com/Ezibenroc/calibration_analysis/blob/a5953da5f3248226c44906751aaba5479e1b8f16/dahu/blas/expfile_influence.ipynb][notebook]] (cells 8,
9, 10, 21 and 22) using these [[https://github.com/Ezibenroc/calibration_analysis/tree/a5953da5f3248226c44906751aaba5479e1b8f16/dahu/blas/expfile_influence/4][archives]]. Relevant entry in my journal: 2020-12-04.
**** Temporal evolution of the average performance and the DRAM power consumption
#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(ggplot2)
library(anytime)
library(patchwork)
library(scales)
library(ggrepel)
library(ggbeeswarm)
set.seed(42)

do_plot = function(frame, y_val, y_label) {
    avg = frame %>%
        group_by(exp_kind) %>%
        summarise(val=mean(.data[[y_val]]))
    basic_plot = frame %>%
        ggplot() +
        aes_string(y=y_val, color="exp_kind", fill="exp_kind") +
        ylab(y_label) +
        theme_bw() +
        expand_limits(y=c(min(frame[[y_val]]), max(frame[[y_val]]))) +
        scale_fill_brewer(type='qual', palette='Dark2', guide=guide_legend()) +
        scale_color_brewer(type='qual', palette='Dark2', guide=guide_legend()) +
        labs(fill='Experiment') +
        theme(legend.spacing.y = unit(0., 'cm'))+
        guides(fill = guide_legend(label.position = "bottom", override.aes = list(alpha=1, shape=21, size=4)))

    p1 = basic_plot +
        geom_boxplot(aes(x=exp_kind), outlier.alpha=0, alpha=0.3) +
        theme(legend.position='none') +
        ylab(y_label) +
        theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.title.x=element_blank())
    p2 = basic_plot +
        geom_point(shape=21, aes(x=start_time), color="black", size=5, alpha=0.7, stroke=0) +
        scale_x_datetime(breaks = date_breaks("1 day")) +
        theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.y=element_blank()) +
        xlab('Timestamp')

    layout <- c(
        area(t = 0, l = 0, b = 20, r = 10),
        area(t = 0, l = 11, b = 20, r = 51),
        area(t=21, l=0, b=22, r=20)
    )
    plot = p1 + p2 + guide_area() + plot_layout(widths=c(3,12,2), guides = 'collect')
    return(plot)
}
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = read.csv("data/experiment/randomizing_sizes/fixing_K/dgemm_agr_data.csv") %>%
    mutate(exp_kind=as.character(exp_kind)) %>%
    mutate(exp_kind=ifelse(exp_kind=="shuffled", "{128,256,512}", exp_kind)) %>%
    filter(node==5, cpu==1) %>%
    mutate(start_time=anytime(start_time)) %>%
    do_plot("avg_gflops", "Average performance (Gflop/s)")
ggsave(filename='img/experiment/randomizing_sizes/fixing_K/average_performance.pdf', plot=plot, width=7, height=3.2)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figureQWTqrD.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = read.csv("data/experiment/randomizing_sizes/fixing_K/dgemm_agr_monitoring.csv") %>%
    mutate(exp_kind=as.character(exp_kind)) %>%
    mutate(exp_kind=ifelse(exp_kind=="shuffled", "{128,256,512}", exp_kind)) %>%
    filter(node==5, cpu==1, kind=="temperature") %>%
    mutate(start_time=anytime(start_time)) %>%
    do_plot("value", "Average CPU temperature (°C)")
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figureahP4JV.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = read.csv("data/experiment/randomizing_sizes/fixing_K/dgemm_agr_monitoring.csv") %>%
    mutate(exp_kind=as.character(exp_kind)) %>%
    mutate(exp_kind=ifelse(exp_kind=="shuffled", "{128,256,512}", exp_kind)) %>%
    filter(node==5, cpu==1, kind=="frequency") %>%
    mutate(start_time=anytime(start_time)) %>%
    do_plot("value", "Average CPU frequency (GHz)")
ggsave(filename='img/experiment/randomizing_sizes/fixing_K/average_frequency.pdf', plot=plot, width=7, height=3.2)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figureJFpNkW.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = read.csv("data/experiment/randomizing_sizes/fixing_K/dgemm_agr_monitoring.csv") %>%
    mutate(exp_kind=as.character(exp_kind)) %>%
    mutate(exp_kind=ifelse(exp_kind=="shuffled", "{128,256,512}", exp_kind)) %>%
    filter(node==5, cpu==1, kind=="power", subgroup=="package") %>%
    mutate(start_time=anytime(start_time)) %>%
    do_plot("value", "Average CPU power consumption (W)")
ggsave(filename='img/experiment/randomizing_sizes/fixing_K/average_power_CPU.pdf', plot=plot, width=7, height=3.2)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figureIwYiRc.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = read.csv("data/experiment/randomizing_sizes/fixing_K/dgemm_agr_monitoring.csv") %>%
    mutate(exp_kind=as.character(exp_kind)) %>%
    mutate(exp_kind=ifelse(exp_kind=="shuffled", "{128,256,512}", exp_kind)) %>%
    filter(node==5, cpu==1, kind=="power", subgroup=="dram") %>%
    mutate(start_time=anytime(start_time)) %>%
    do_plot("value", "Average DRAM power consumption (W)")
ggsave(filename='img/experiment/randomizing_sizes/fixing_K/average_power_DRAM.pdf', plot=plot, width=7, height=3.2)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figuremyzJjF.png]]
**** Distribution of the regression coefficients
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
library(dplyr)
library(ggplot2)
library(anytime)
library(patchwork)
library(scales)
library(ggrepel)

plot_bivariate <- function(df, x_col, y_col, color_col) {
    density_plot = ggplot() +
        aes_string(color=color_col, group=color_col) +
        geom_density(data=df) +
        theme_void()

    plot_top = density_plot +
        aes_string(x=x_col)
    plot_right = density_plot +
        aes_string(x=y_col) +
        coord_flip()

    text = df %>%
        group_by_(color_col) %>%
        summarise_all(funs(mean))

    scatter_plot = ggplot(df) +
        aes_string(x=x_col, y=y_col, color=color_col, group=color_col) +
        geom_point() +
        stat_ellipse() +
# Uncomment the following line to put labels on the ellipses
        geom_label(data=text, aes_string(label=color_col), alpha=0.7) +
        theme_minimal() +
        scale_x_continuous(breaks = scales::pretty_breaks(n = 3)) +
        scale_y_continuous(breaks = scales::pretty_breaks(n = 3)) +
        xlab(toupper(x_col)) +
        ylab(toupper(y_col))

    return(
        plot_top + plot_spacer() + scatter_plot + plot_right +
        plot_layout(widths = c(4, 1), heights = c(1, 4)) &
        scale_color_brewer(palette="Dark2", type="qual") &
        theme(legend.position='none')
    )
}

plot = read.csv("data/experiment/randomizing_sizes/fixing_K/dgemm_agr_data.csv") %>%
    mutate(exp_kind=as.character(exp_kind)) %>%
    mutate(exp_kind=ifelse(exp_kind=="shuffled", "{128,256,512}", exp_kind)) %>%
    filter(node==5, cpu==1) %>%
    mutate(cpu_id=interaction(node, cpu), group=interaction(cpu_id, exp_kind)) %>%
    plot_bivariate("mnk", "nk", "exp_kind")
#ggsave(filename='img/experiment/randomizing_sizes/expfile/average_distribution.pdf', plot=plot, width=7, height=5)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figureAtNzXZ.png]]
**** Individual durations
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
library(dplyr)
library(ggplot2)
library(ggbeeswarm)

plot = read.csv("data/experiment/randomizing_sizes/fixing_K/dgemm_raw_data.csv") %>%
    filter(node==5, cpu==1) %>%
    mutate(MNK=as.numeric(m)*as.numeric(n)*as.numeric(k), gflops=2*MNK*1e-9/duration) %>%
    filter(MNK >= 1000) %>% # removing the smallest matrices
    filter(m != 2048 | n != 2048 | k != 2048) %>% # removing this special point
    mutate(exp_kind=
        ifelse(grepl("128", expfile, fixed=T), "128",
        ifelse(grepl("256", expfile, fixed=T), "256",
        ifelse(grepl("512", expfile, fixed=T), "512",
        ifelse(grepl("random", expfile, fixed=T), "random",
        ifelse(grepl("shuffled", expfile, fixed=T), "{128,256,512}",
    "NA")))))
    ) %>%
    mutate(K_val=ifelse(exp_kind=="random", "other", as.character(k))) %>% {(
    ggplot(.) +
    aes(x=exp_kind, y=gflops, color=K_val, group=interaction(exp_kind, K_val)) +
    #geom_point(alpha=0.1) +
    geom_quasirandom(alpha=0.5, dodge.width=1) +
    geom_boxplot(alpha=0, outlier.alpha=0, color="black", width=0.5, position=position_dodge(width=1)) +
    geom_label(aes(label=exp_kind, group=NULL, color=NULL, fill=exp_kind), data=data.frame(exp_kind=pull(.,exp_kind)%>%unique(), gflops=pull(., gflops)%>%min()*0.8), show.legend=F) +
    scale_color_manual(values=c("128"="#d95f02", "256"="#7570b3", "512"="#e7298a", "other"="#66a61e")) +
    scale_fill_brewer(palette="Dark2") +
    geom_abline(slope=6.7e-11) +
    guides(colour = guide_legend(override.aes = list(alpha = 1, size=4))) +
    theme_bw() +
    theme(axis.text.x=element_blank()) +
    coord_cartesian(ylim = c(pull(.,gflops)%>%min(), pull(.,gflops)%>%max()), clip="off") +
    theme(plot.margin = unit(c(5.5,5.5,20,5.5), "pt")) +
    theme(axis.title.x = element_text(vjust=-5)) +
    xlab("Experiment") +
    ylab("Performance (Gflop/s)") +
    labs(color="K value")
    )}
ggsave(filename='img/experiment/randomizing_sizes/fixing_K/raw_data.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-Zo95a7/figure4UFWGx.png]]

** Randomizing the data
The CSV files used in this section have been dumped by the notebooks [[https://github.com/Ezibenroc/presentations/blob/30849c839426038fe6d4840cd4403f0d422136c3/2020/jlesc/fig/notebook_generation_method.ipynb][here]] (using
the archives from this [[https://github.com/Ezibenroc/presentations/tree/30849c839426038fe6d4840cd4403f0d422136c3/2020/jlesc/fig/exp_data/1][directory]]) and [[https://github.com/Ezibenroc/presentations/blob/30849c839426038fe6d4840cd4403f0d422136c3/2020/jlesc/fig/notebook_mask_size.ipynb][here]] (using the archives from this
[[https://github.com/Ezibenroc/presentations/tree/30849c839426038fe6d4840cd4403f0d422136c3/2020/jlesc/fig/exp_data/2][directory]]). You can also see the entries in my journal from 2019/10/25 and
2019/10/29.
*** Generation method
#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(ggplot2)
library(patchwork)

do_plot = function(frame, y_val, y_label, col_val) {
    col_label = gsub("_", " ", col_val)
    col_label = paste(toupper(substr(col_label, 1, 1)), substr(col_label, 2, nchar(col_label)), sep="")
    frame$x_val = ifelse(frame[[col_val]]=="random", "rand",
                  ifelse(frame[[col_val]]=="sequential", "seq", as.character(frame[[col_val]])))
    basic_plot = frame %>%
        ggplot() +
        aes_string(y=y_val, fill=col_val) +
        ylab(y_label) +
        labs(fill=col_label) +
        theme_bw() +
        scale_y_continuous(labels = scales::number_format(accuracy = 0.001)) +
        guides(fill = guide_legend(override.aes = list(alpha=1, shape=21, size=4)))

    p1 = basic_plot +
        geom_boxplot(aes(x=x_val), outlier.alpha=0, alpha=1) +
        theme(legend.position='none') +
#        theme(axis.text.x=element_blank()) +
        xlab(col_label)
    p2 = basic_plot +
        geom_point(shape=21, aes(x=timestamp), size=1, alpha=0.3, stroke=0) +
        theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.y=element_blank()) +
        xlab('Timestamp (s)')

    plot = p1 + p2 + plot_layout(widths=c(1,2), guides = 'collect')
    return(plot)
}
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
read.csv("data/experiment/bit-flips/generation_method_perf.csv") %>%
    do_plot("duration", "Duration (s)", "matrix_content") &
    scale_fill_brewer(type='qual', palette='Dark2') -> plot
ggsave(filename='img/experiment/bit-flips/generation_method_perf.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-bipihK/figureEnXWKR.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
read.csv("data/experiment/bit-flips/generation_method_freq.csv") %>%
    do_plot("frequency", "Frequency (GHz)", "matrix_content") &
    scale_fill_brewer(type='qual', palette='Dark2') -> plot
ggsave(filename='img/experiment/bit-flips/generation_method_freq.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-bipihK/figure5qkT5W.png]]

*** Mask size
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
read.csv("data/experiment/bit-flips/mask_size_perf.csv") %>%
    mutate(mask_size=factor(mask_size)) %>%
    do_plot("duration", "Duration (s)", "mask_size") &
    scale_fill_brewer(type='seq', palette='Blues') -> plot
ggsave(filename='img/experiment/bit-flips/mask_size_perf.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-bipihK/figureic0oHe.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
read.csv("data/experiment/bit-flips/mask_size_freq.csv") %>%
    mutate(mask_size=factor(mask_size)) %>%
    do_plot("frequency", "Frequency (GHz)", "mask_size") &
    scale_fill_brewer(type='seq', palette='Blues') -> plot
ggsave(filename='img/experiment/bit-flips/mask_size_freq.png', plot=plot, width=7, height=3, dpi=200)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-bipihK/figureZHGqH0.png]]
** Beware of experimental conditions
The CSV file used in this section has been dumped by this [[https://github.com/Ezibenroc/calibration_analysis/blob/7170f426a97e3dc45e19e03c4ca2b7d988d3d466/dahu/mpi/comparison_ring_classical.ipynb][notebook]] (cell 5)
using [[https://github.com/Ezibenroc/calibration_analysis/tree/7170f426a97e3dc45e19e03c4ca2b7d988d3d466/dahu/mpi/250MB/1][these archives]].
*** Plot drawing
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
library(dplyr)
library(ggplot2)

bw  = 12.5*1e9
lat = 0.1*1e-6

plot = read.csv("data/experiment/experimental_conditions/communication_computation_interference.csv") %>%
    mutate(background=factor(background, levels=c("Idle", "DGEMM", "MPI_Iprobe", "MPI_Iprobe & DGEMM"))) %>% {(
    ggplot(.) +
        aes(x=background, y=duration, color=experiment) +
        geom_boxplot(outlier.alpha=0, size=1, position=position_dodge(width=0.5)) +
        theme_bw() +
        expand_limits(y=0) +
        scale_color_brewer(type='qual', palette='Dark2') +
        labs(x='Computations on the node', y='Duration (seconds)', color='Locality') +
        geom_hline(yintercept=pull(., msg_size) %>% max() / bw + lat, linetype='dashed') +
        annotate('label', x=3.5, y=pull(., msg_size) %>% max() / bw + lat, label='Expected duration on a 100 Gbps link') +
        theme(legend.position=c(0.11, 0.77), legend.background=element_rect(color='black'), text=element_text(size=16))
)}
ggsave(filename='img/experiment/experimental_conditions/communication_computation_interference.pdf', plot=plot, width=7, height=3)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-aL5QFP/figureKi3dYk.png]]

** Performance non-regression tests
*** State of the art (StarPU)
I regenerate the figure for the performance tests of StarPU (see the slide 55 of
the [[https://hal.inria.fr/hal-02943753/file/20-02-15-siampp-seattle.pdf][SiamPP presentation]]). A recent version of the dataset is available [[https://files.inria.fr/starpu/testing/morse/master/morse_spotrf_seq.html][online]],
but all the data from 2014 to 2020 has been discarded, which is unfortunate.

I obtained the old dataset by an email from Nathalie Furmento on
[2021-01-06 Wed].

First, let's clean this mess. The number of columns in the files changes over
time, so we cannot directly load it as a CSV and select the desired
columns. From what I understood, the duration and gflops columns are the ones
directly to the left of the symbols "+-", so let's use this for the selection.

#+begin_src python :results output :session *python* :exports both
path = "data/experiment/non_regression/state_of_art/"
files = {
    "results_master_HEAD_spotrf_seq_sim.dat": "simulation",
    "results_master_HEAD_spotrf_seq.dat"    : "reality",
}
for old_f, new_f in files.items():
    with open(path + old_f) as old:
        with open(path + new_f + ".csv", "w") as new:
            new.write(f"date,duration,gflops,mode\n")
            for line in old:
                line = line.split()
                date = line[0]
                try:
                    i = line.index("+-")
                except ValueError:
                    continue
                time, gflops = line[i-2:i]
                try:
                    float(time) + float(gflops)
                except ValueError:
                    continue
                new.write(f"{date},{time},{gflops},{new_f}\n")
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
library(dplyr)
library(ggplot2)
library(anytime)
df = rbind(
    read.csv("data/experiment/non_regression/state_of_art/reality.csv"),
    read.csv("data/experiment/non_regression/state_of_art/simulation.csv")
) %>%
    mutate(date=anytime(date)) %>%
    mutate(mode=stringr::str_to_title(mode))
str(df)
#+end_src

#+RESULTS:
: 
: 'data.frame':	1433 obs. of  4 variables:
:  $ date    : POSIXct, format: "2014-04-01" "2014-04-02" ...
:  $ duration: num  15.2 14.7 16.1 15.3 16.2 ...
:  $ gflops  : num  1239 1284 1173 1236 1165 ...
:  $ mode    : chr  "Reality" "Reality" "Reality" "Reality" ...

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
plot = ggplot(df) +
    aes(x=date, y=gflops, color=mode) +
    geom_point(shape=21, alpha=0.5) +
    geom_line(alpha=0.2, show.legend=F) +
    guides(colour = guide_legend(override.aes = list(alpha = 1, shape=16, size=4))) +
    theme_bw() +
    xlab("Date") + ylab("Performance (Gflop/s)") +
    geom_vline(xintercept=anytime("2017-12-15"), linetype="dashed") +
    annotate("label", x=anytime("2017-12-15"), y=3000, label="Hardware migration") +
    expand_limits(y=0) +
    scale_color_brewer(palette="Dark2") +
    theme(plot.margin = unit(c(5.5,12,5.5,5.5), "pt")) +  # sadly required, otherwise the "2020" gets cut off...
    theme(legend.title=element_blank(), legend.position=c(0.1, 0.82), legend.background=element_rect(color="black", size=0.3))
ggsave(filename='img/experiment/non_regression/state_of_art/starpu_new.pdf', plot=plot, width=7, height=3)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-bJC7jD/figuresNlJkO.png]]

*** Statistical tests
**** Single point
Relevant links for the plots:
- https://github.com/tidyverse/ggplot2/blob/master/R/stat-ellipse.R
- https://stackoverflow.com/questions/27382145/fine-tuning-stat-ellipse-in-ggplot2/
- https://stackoverflow.com/questions/33678862/shading-a-region-with-stat-function-and-ggplot
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
library(dplyr)
library(ggplot2)
library(patchwork)
library(MASS)  # mvrnorm function, to sample multivariate normal variables
set.seed(42)
mu = c(10, 20)
rho = -0.7
correlation = matrix(c(1, rho, rho, 1), 2)
std = c(1, 10)
sigma =  diag(std) %*% correlation %*% diag(std)
N = 1000
confidence = 0.995


radius = sqrt(2 * stats::qf(confidence, 2, Inf))

calculate_ellipse <- function(center, shape, radius, segments){
# Adapted from https://github.com/tidyverse/ggplot2/blob/master/R/stat-ellipse.R
    chol_decomp <- chol(shape)
    angles <- (0:segments) * 2 * pi/segments
    unit.circle <- cbind(cos(angles), sin(angles))
    ellipse <- t(center + radius * t(unit.circle %*% chol_decomp))
    colnames(ellipse) <- c("x","y")
    as.data.frame(ellipse)
}

normal_shade <- function(mean, sd, x, radius){
    inf = mean-radius
    sup = mean+radius
    y = dnorm(x, mean=mean, sd=sd)
    y[x < inf | x > sup] = NA
    return(y)
}

draw_plot <- function(center, shape, radius, new_points, new_point_size, draw_new_point_mean=F, old_radius=NA, nb_points=1000, seed=42) {
    set.seed(seed)
    data = mvrnorm(n=nb_points, mu=center, Sigma=shape)
    df = data.frame(x=data[,1], y=data[,2])

    base_plot = df %>%
        ggplot() +
        theme_minimal() +
        theme(axis.title.x=element_blank(), axis.title.y=element_blank()) +
        theme(axis.text.x=element_blank(), axis.text.y=element_blank())

    base_col = "#2196F3"
    base_alpha = 0.3

    l=4
    scatter_plot = base_plot +
        aes(x=x, y=y) +
        geom_point() +
        geom_path(data=calculate_ellipse(center, shape, old_radius, 1000), colour=base_col, linetype="dashed") +
        geom_polygon(data=calculate_ellipse(center, shape, radius, 1000), colour=base_col, fill=base_col, alpha=base_alpha) +
        expand_limits(x=c(center[1]-l*std[1], center[1]+l*std[1])) +
        expand_limits(y=c(center[2]-l*std[2], center[2]+l*std[2])) +
        geom_point(data=new_points, aes(fill=type), size=new_point_size, shape=21) +
        scale_fill_brewer(palette="Dark2") +
        scale_color_brewer(palette="Dark2") +
        theme(legend.position="none")
    if(draw_new_point_mean) {
        scatter_plot = scatter_plot +
            geom_point(data=new_points %>% group_by(type) %>% summarise(x=mean(x), y=mean(y)), aes(color=type), shape=4, size=new_point_size*2, stroke=new_point_size-1)
    }

    plot_top = base_plot +
        aes(x=x) +
        expand_limits(x=c(center[1]-l*std[1], center[1]+l*std[1])) +
        theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank()) +
        geom_vline(xintercept=c(center[1]-radius*std[1], center[1]+radius*std[1]), color=base_col) +
        geom_vline(xintercept=c(center[1]-old_radius*std[1], center[1]+old_radius*std[1]), color=base_col, linetype="dashed") +
        stat_function(fun = dnorm, n = 1000, args = list(mean=center[1], sd=std[1])) +
        stat_function(fun = normal_shade, n = 1000, args = list(mean=center[1], sd=std[1], radius=radius*std[1]), geom="area", fill=base_col, alpha=base_alpha)

    plot_right = base_plot +
        aes(x=y) +
        expand_limits(x=c(center[2]-l*std[2], center[2]+l*std[2])) +
        theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
        geom_vline(xintercept=c(center[2]-radius*std[2], center[2]+radius*std[2]), color=base_col) +
        geom_vline(xintercept=c(center[2]-old_radius*std[2], center[2]+old_radius*std[2]), color=base_col, linetype="dashed") +
        stat_function(fun = dnorm, n = 1000, args = list(mean=center[2], sd=std[2])) +
        stat_function(fun = normal_shade, n = 1000, args = list(mean=center[2], sd=std[2], radius=radius*std[2]), geom="area", fill=base_col, alpha=base_alpha) +
        coord_flip()

    plot = plot_top + plot_spacer() + scatter_plot + plot_right +
            plot_layout(widths = c(4, 1), heights = c(1, 4))
    return(plot)
}

new_obs = rbind(
    data.frame(x=mu[1]+std[1], y=mu[2]-2*std[2], type="good"),
    data.frame(x=mu[1]+std[1], y=mu[2]+2*std[2], type="bad")
)

plot = draw_plot(mu, sigma, radius, new_obs, new_point_size=6)
ggsave(filename='img/experiment/non_regression/statistics/single_point.pdf', plot=plot, width=7, height=5)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-LlZHec/figureLoLR0V.png]]
**** Multiple points
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
set.seed(41)

mx = mu[1]
my = mu[2]
sx = std[1]
sy = std[2]
new_obs = setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("x", "y", "type"))
new_obs[1,]  = c(mx+sx, my+sy, "good")
new_obs[2,]  = c(mx+sx, my-sy, "bad")
new_obs[3,]  = c(mx-1.7*sx, my+1.7*sy, "good")
new_obs[4,]  = c(mx-sx, my-sy, "bad")
new_obs[5,]  = c(mx+sx, my-sy, "good")
new_obs[6,]  = c(mx-sx, my-sy/2, "bad")
new_obs[7,]  = c(mx-sx, my-sy, "good")
new_obs[8,]  = c(mx+sx/2, my-sy/2, "bad")
new_obs[9,]  = c(mx+sx*2, my-sy*2, "good")
new_obs[10,] = c(mx+sx*2, my-sy*2, "bad")
new_obs$x = as.numeric(new_obs$x) + rnorm(n=nrow(new_obs), mean=0, sd=sx/5)
new_obs$y = as.numeric(new_obs$y) + rnorm(n=nrow(new_obs), mean=0, sd=sy/5)
new_obs$type = factor(new_obs$type, le=c("good", "bad"))

nb_points = nrow(new_obs) / 2
# We compute only an approximation of the radius for multiple points, since we use Inf for the number of previous observations.
# In reality, we should multiply it by N/(N+1) * (N+nb_points)/(N*nb_points) = (N+nb_points)/((N+1)*nb_points)
radius_multiple = sqrt(2 * stats::qf(confidence, 2, Inf)) / nb_points

plot = draw_plot(mu, sigma, radius_multiple, new_obs, old_radius=radius, new_point_size=3, draw_new_point_mean=T)
ggsave(filename='img/experiment/non_regression/statistics/several_points.pdf', plot=plot, width=7, height=5)
plot
#+end_src

#+RESULTS:
[[file:/tmp/babel-LlZHec/figureHwoaVK.png]]

